{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/workspace/cache/\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME=/workspace/cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEBUG_MODE = False\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install transformer_lens\n",
    "    %pip install torchtyping\n",
    "    # Install my janky personal plotting utils\n",
    "    %pip install git+https://github.com/neelnanda-io/neel-plotly.git\n",
    "    # Install another version of node that makes PySvelte work way faster\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "    # Needed for PySvelte to work, v3 came out and broke things...\n",
    "    %pip install typeguard==2.13.3\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "\n",
    "if IN_COLAB or not DEBUG_MODE:\n",
    "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import torch\n",
    "import einops\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import pprint\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from IPython.display import HTML\n",
    "from functools import partial\n",
    "import tqdm.notebook as tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from circuitsvis.attention import attention_heads\n",
    "from IPython.display import HTML, IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 3e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\":\"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        d_hidden = cfg[\"d_mlp\"] * cfg[\"dict_mult\"]\n",
    "        d_mlp = cfg[\"d_mlp\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
    "\n",
    "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(\"cuda\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cent = x - self.b_dec\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def remove_parallel_component_of_grads(self):\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "\n",
    "    # def get_version(self):\n",
    "    #     return 1+max([int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)])\n",
    "\n",
    "    # def save(self):\n",
    "    #     version = self.get_version()\n",
    "    #     torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
    "    #     with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
    "    #         json.dump(cfg, f)\n",
    "    #     print(\"Saved as version\", version)\n",
    "\n",
    "    # def load(cls, version):\n",
    "    #     cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
    "    #     pprint.pprint(cfg)\n",
    "    #     self = cls(cfg=cfg)\n",
    "    #     self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
    "    #     return self\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(cls, version):\n",
    "        \"\"\"\n",
    "        Loads the saved autoencoder from HuggingFace.\n",
    "\n",
    "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
    "\n",
    "        version 25 is the final checkpoint of the first autoencoder run,\n",
    "        version 47 is the final checkpoint of the second autoencoder run.\n",
    "        \"\"\"\n",
    "        if version==\"run1\":\n",
    "            version = 25\n",
    "        elif version==\"run2\":\n",
    "            version = 47\n",
    "\n",
    "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_hook(mlp_post, hook, encoder):\n",
    "    mlp_post_reconstr = encoder(mlp_post)[1]\n",
    "    return mlp_post_reconstr\n",
    "\n",
    "def mean_ablate_hook(mlp_post, hook):\n",
    "    mlp_post[:] = mlp_post.mean([0, 1])\n",
    "    return mlp_post\n",
    "\n",
    "def zero_ablate_hook(mlp_post, hook):\n",
    "    mlp_post[:] = 0.\n",
    "    return mlp_post\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_recons_loss(num_batches=5, local_encoder=None):\n",
    "    if local_encoder is None:\n",
    "        local_encoder = encoder\n",
    "    loss_list = []\n",
    "    for i in range(num_batches):\n",
    "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
    "        loss = model(tokens, return_type=\"loss\")\n",
    "        recons_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), partial(replacement_hook, encoder=local_encoder))])\n",
    "        # mean_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), mean_ablate_hook)])\n",
    "        zero_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), zero_ablate_hook)])\n",
    "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
    "    losses = torch.tensor(loss_list)\n",
    "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
    "\n",
    "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
    "    score = ((zero_abl_loss - recons_loss)/(zero_abl_loss - loss))\n",
    "    print(f\"Reconstruction Score: {score:.2%}\")\n",
    "    # print(f\"{((zero_abl_loss - mean_abl_loss)/(zero_abl_loss - loss)).item():.2%}\")\n",
    "    return score, loss, recons_loss, zero_abl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency\n",
    "@torch.no_grad()\n",
    "def get_freqs(num_batches=25, local_encoder=None):\n",
    "    if local_encoder is None:\n",
    "        local_encoder = encoder\n",
    "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).cuda()\n",
    "    total = 0\n",
    "    for i in tqdm.trange(num_batches):\n",
    "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
    "\n",
    "        _, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
    "        mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
    "        mlp_acts = mlp_acts.reshape(-1, d_mlp)\n",
    "\n",
    "        hidden = local_encoder(mlp_acts)[2]\n",
    "\n",
    "        act_freq_scores += (hidden > 0).sum(0)\n",
    "        total+=hidden.shape[0]\n",
    "    act_freq_scores /= total\n",
    "    num_dead = (act_freq_scores==0).float().mean()\n",
    "    print(\"Num dead\", num_dead)\n",
    "    return act_freq_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Feature Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import escape\n",
    "import colorsys\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "SPACE = \"·\"\n",
    "NEWLINE=\"↩\"\n",
    "TAB = \"→\"\n",
    "\n",
    "def create_html(strings, values, max_value=None, saturation=0.5, allow_different_length=False, return_string=False):\n",
    "    # escape strings to deal with tabs, newlines, etc.\n",
    "    escaped_strings = [escape(s, quote=True) for s in strings]\n",
    "    processed_strings = [\n",
    "        s.replace(\"\\n\", f\"{NEWLINE}<br/>\").replace(\"\\t\", f\"{TAB}&emsp;\").replace(\" \", \"&nbsp;\")\n",
    "        for s in escaped_strings\n",
    "    ]\n",
    "\n",
    "    if isinstance(values, torch.Tensor) and len(values.shape)>1:\n",
    "        values = values.flatten().tolist()\n",
    "\n",
    "    if not allow_different_length:\n",
    "        assert len(processed_strings) == len(values)\n",
    "\n",
    "    # scale values\n",
    "    if max_value is None:\n",
    "        max_value = max(max(values), -min(values))+1e-3\n",
    "    scaled_values = [v / max_value * saturation for v in values]\n",
    "\n",
    "    # create html\n",
    "    html = \"\"\n",
    "    for i, s in enumerate(processed_strings):\n",
    "        if i<len(scaled_values):\n",
    "            v = scaled_values[i]\n",
    "        else:\n",
    "            v = 0\n",
    "        if v < 0:\n",
    "            hue = 0  # hue for red in HSV\n",
    "        else:\n",
    "            hue = 0.66  # hue for blue in HSV\n",
    "        rgb_color = colorsys.hsv_to_rgb(\n",
    "            hue, v, 1\n",
    "        )  # hsv color with hue 0.66 (blue), saturation as v, value 1\n",
    "        hex_color = \"#%02x%02x%02x\" % (\n",
    "            int(rgb_color[0] * 255),\n",
    "            int(rgb_color[1] * 255),\n",
    "            int(rgb_color[2] * 255),\n",
    "        )\n",
    "        html += f'<span style=\"background-color: {hex_color}; border: 1px solid lightgray; font-size: 16px; border-radius: 3px;\">{s}</span>'\n",
    "    if return_string:\n",
    "        return html\n",
    "    else:\n",
    "        display(HTML(html))\n",
    "\n",
    "def basic_feature_vis(text, feature_index, max_val=0):\n",
    "    feature_in = encoder.W_enc[:, feature_index]\n",
    "    feature_bias = encoder.b_enc[feature_index]\n",
    "    _, cache = model.run_with_cache(text, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
    "    mlp_acts = cache[utils.get_act_name(\"post\", 0)][0]\n",
    "    feature_acts = F.relu((mlp_acts - encoder.b_dec) @ feature_in + feature_bias)\n",
    "    if max_val==0:\n",
    "        max_val = max(1e-7, feature_acts.max().item())\n",
    "        # print(max_val)\n",
    "    # if min_val==0:\n",
    "    #     min_val = min(-1e-7, feature_acts.min().item())\n",
    "    return basic_token_vis_make_str(text, feature_acts, max_val)\n",
    "def basic_token_vis_make_str(strings, values, max_val=None):\n",
    "    if not isinstance(strings, list):\n",
    "        strings = model.to_str_tokens(strings)\n",
    "    values = utils.to_numpy(values)\n",
    "    if max_val is None:\n",
    "        max_val = values.max()\n",
    "    # if min_val is None:\n",
    "    #     min_val = values.min()\n",
    "    header_string = f\"<h4>Max Range <b>{values.max():.4f}</b> Min Range: <b>{values.min():.4f}</b></h4>\"\n",
    "    header_string += f\"<h4>Set Max Range <b>{max_val:.4f}</b></h4>\"\n",
    "    # values[values>0] = values[values>0]/ma|x_val\n",
    "    # values[values<0] = values[values<0]/abs(min_val)\n",
    "    body_string = create_html(strings, values, max_value=max_val, return_string=True)\n",
    "    return header_string + body_string\n",
    "# display(HTML(basic_token_vis_make_str(tokens[0, :10], mlp_acts[0, :10, 7], 0.1)))\n",
    "# # %%\n",
    "# The `with gr.Blocks() as demo:` syntax just creates a variable called demo containing all these components\n",
    "import gradio as gr\n",
    "try:\n",
    "    demos[0].close()\n",
    "except:\n",
    "    pass\n",
    "demos = [None]\n",
    "def make_feature_vis_gradio(feature_id, starting_text=None, batch=None, pos=None):\n",
    "    if starting_text is None:\n",
    "        starting_text = model.to_string(all_tokens[batch, 1:pos+1])\n",
    "    try:\n",
    "        demos[0].close()\n",
    "    except:\n",
    "        pass\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.HTML(value=f\"Hacky Interactive Neuroscope for gelu-1l\")\n",
    "        # The input elements\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text = gr.Textbox(label=\"Text\", value=starting_text)\n",
    "                # Precision=0 makes it an int, otherwise it's a float\n",
    "                # Value sets the initial default value\n",
    "                feature_index = gr.Number(\n",
    "                    label=\"Feature Index\", value=feature_id, precision=0\n",
    "                )\n",
    "                # # If empty, these two map to None\n",
    "                max_val = gr.Number(label=\"Max Value\", value=None)\n",
    "                # min_val = gr.Number(label=\"Min Value\", value=None)\n",
    "                inputs = [text, feature_index, max_val]\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                # The output element\n",
    "                out = gr.HTML(label=\"Neuron Acts\", value=basic_feature_vis(starting_text, feature_id))\n",
    "        for inp in inputs:\n",
    "            inp.change(basic_feature_vis, inputs, out)\n",
    "    demo.launch(share=True)\n",
    "    demos[0] = demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Top Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE = \"·\"\n",
    "NEWLINE=\"↩\"\n",
    "TAB = \"→\"\n",
    "def process_token(s):\n",
    "    if isinstance(s, torch.Tensor):\n",
    "        s = s.item()\n",
    "    if isinstance(s, np.int64):\n",
    "        s = s.item()\n",
    "    if isinstance(s, int):\n",
    "        s = model.to_string(s)\n",
    "    s = s.replace(\" \", SPACE)\n",
    "    s = s.replace(\"\\n\", NEWLINE+\"\\n\")\n",
    "    s = s.replace(\"\\t\", TAB)\n",
    "    return s\n",
    "\n",
    "def process_tokens(l):\n",
    "    if isinstance(l, str):\n",
    "        l = model.to_str_tokens(l)\n",
    "    elif isinstance(l, torch.Tensor) and len(l.shape)>1:\n",
    "        l = l.squeeze(0)\n",
    "    return [process_token(s) for s in l]\n",
    "\n",
    "def process_tokens_index(l):\n",
    "    if isinstance(l, str):\n",
    "        l = model.to_str_tokens(l)\n",
    "    elif isinstance(l, torch.Tensor) and len(l.shape)>1:\n",
    "        l = l.squeeze(0)\n",
    "    return [f\"{process_token(s)}/{i}\" for i,s in enumerate(l)]\n",
    "\n",
    "def create_vocab_df(logit_vec, make_probs=False, full_vocab=None):\n",
    "    if full_vocab is None:\n",
    "        full_vocab = process_tokens(model.to_str_tokens(torch.arange(model.cfg.d_vocab)))\n",
    "    vocab_df = pd.DataFrame({\"token\": full_vocab, \"logit\": utils.to_numpy(logit_vec)})\n",
    "    if make_probs:\n",
    "        vocab_df[\"log_prob\"] = utils.to_numpy(logit_vec.log_softmax(dim=-1))\n",
    "        vocab_df[\"prob\"] = utils.to_numpy(logit_vec.softmax(dim=-1))\n",
    "    return vocab_df.sort_values(\"logit\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Token DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_flatten(nested_list):\n",
    "    return [x for y in nested_list for x in y]\n",
    "def make_token_df(tokens, len_prefix=5, len_suffix=1):\n",
    "    str_tokens = [process_tokens(model.to_str_tokens(t)) for t in tokens]\n",
    "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
    "\n",
    "    context = []\n",
    "    batch = []\n",
    "    pos = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        # context.append([])\n",
    "        # batch.append([])\n",
    "        # pos.append([])\n",
    "        # label.append([])\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
    "            if p==tokens.shape[1]-1:\n",
    "                suffix = \"\"\n",
    "            else:\n",
    "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
    "            batch.append(b)\n",
    "            pos.append(p)\n",
    "            label.append(f\"{b}/{p}\")\n",
    "    # print(len(batch), len(pos), len(context), len(label))\n",
    "    return pd.DataFrame(dict(\n",
    "        str_tokens=list_flatten(str_tokens),\n",
    "        unique_token=list_flatten(unique_token),\n",
    "        context=context,\n",
    "        batch=batch,\n",
    "        pos=pos,\n",
    "        label=label,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n",
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "d_head = model.cfg.d_head\n",
    "d_mlp = model.cfg.d_mlp\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder: Finding Higher Frequency Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 4096,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.99,\n",
      " 'buffer_batches': 12288,\n",
      " 'buffer_mult': 384,\n",
      " 'buffer_size': 1572864,\n",
      " 'd_mlp': 2048,\n",
      " 'dict_mult': 8,\n",
      " 'enc_dtype': 'fp32',\n",
      " 'l1_coeff': 0.0003,\n",
      " 'lr': 0.0001,\n",
      " 'model_batch_size': 512,\n",
      " 'num_tokens': 2000000000,\n",
      " 'seed': 52,\n",
      " 'seq_len': 128}\n"
     ]
    }
   ],
   "source": [
    "auto_encoder_run = \"run1\" # @param [\"run1\", \"run2\"]\n",
    "encoder = AutoEncoder.load_from_hf(auto_encoder_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.2575, recons_loss: 3.7475, zero_abl_loss: 8.7729\n",
      "Reconstruction Score: 91.12%\n"
     ]
    }
   ],
   "source": [
    "_ = get_recons_loss(num_batches=20, local_encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36da6a2fb2184c74a3ff24131e1bf610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num dead tensor(6.1035e-05, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "freqs = get_freqs(num_batches = 50, local_encoder = encoder)\n",
    "is_rare = freqs < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A  `('` feature in GELU-1L\n",
    "\n",
    "\n",
    "- 5018 : Cool features that boosts the logits of the modes of communication, head 4 is interesting\n",
    "\n",
    "\n",
    "- 2410 :- polysemantic neuron\n",
    "- 2393 :- Error Token\n",
    "- 1625 :- Blue token \n",
    "- 5090 :- tokens following `of`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature freq: 0.01118\n",
      "Is Rare: False\n"
     ]
    }
   ],
   "source": [
    "feature_id = 5090 # @param {type:\"number\"}\n",
    "batch_size = 128 # @param {type:\"number\"}\n",
    "\n",
    "print(f\"Feature freq: {freqs[feature_id].item():.5f}\")\n",
    "print(f\"Is Rare: {is_rare[feature_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_acts.shape torch.Size([163840, 16384])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    tokens = all_tokens[batch_size*0:batch_size*10]\n",
    "    _, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
    "    mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
    "    mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
    "    loss, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(mlp_acts_flattened)\n",
    "    # This is equivalent to:\n",
    "    # hidden_acts = F.relu((mlp_acts_flattened - encoder.b_dec) @ encoder.W_enc + encoder.b_enc)\n",
    "    print(\"hidden_acts.shape\", hidden_acts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum activating examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dce6d_row0_col3 {\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row0_col4 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row0_col6, #T_dce6d_row6_col3, #T_dce6d_row10_col4 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row1_col3 {\n",
       "  background-color: #c0d4f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row1_col4 {\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row1_col6 {\n",
       "  background-color: #b8122a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row2_col3, #T_dce6d_row8_col3 {\n",
       "  background-color: #dddcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row2_col4 {\n",
       "  background-color: #ead5c9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row2_col6, #T_dce6d_row11_col4 {\n",
       "  background-color: #e57058;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row3_col3 {\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row3_col4 {\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row3_col6 {\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row4_col3, #T_dce6d_row17_col6, #T_dce6d_row18_col6 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row4_col4 {\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row4_col6 {\n",
       "  background-color: #e1dad6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row5_col3, #T_dce6d_row15_col4, #T_dce6d_row19_col6 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row5_col4 {\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row5_col6 {\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row6_col4 {\n",
       "  background-color: #cb3e38;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row6_col6 {\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row7_col3 {\n",
       "  background-color: #f6bea4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row7_col4 {\n",
       "  background-color: #e7745b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row7_col6 {\n",
       "  background-color: #b1cbfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row8_col4 {\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row8_col6, #T_dce6d_row13_col4 {\n",
       "  background-color: #94b6ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row9_col3 {\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row9_col4 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row9_col6 {\n",
       "  background-color: #92b4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row10_col3 {\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row10_col6 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row11_col3 {\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row11_col6 {\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row12_col3 {\n",
       "  background-color: #e3d9d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row12_col4 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row12_col6, #T_dce6d_row13_col6 {\n",
       "  background-color: #7396f5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row13_col3 {\n",
       "  background-color: #be242e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row14_col3 {\n",
       "  background-color: #a6c4fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row14_col4 {\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row14_col6 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row15_col3, #T_dce6d_row16_col3 {\n",
       "  background-color: #c53334;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row15_col6 {\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row16_col4 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row16_col6 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row17_col3 {\n",
       "  background-color: #c0282f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row17_col4 {\n",
       "  background-color: #f7bca1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_dce6d_row18_col3 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row18_col4 {\n",
       "  background-color: #d65244;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row19_col3 {\n",
       "  background-color: #c43032;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_dce6d_row19_col4 {\n",
       "  background-color: #f4987a;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dce6d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dce6d_level0_col0\" class=\"col_heading level0 col0\" >str_tokens</th>\n",
       "      <th id=\"T_dce6d_level0_col1\" class=\"col_heading level0 col1\" >unique_token</th>\n",
       "      <th id=\"T_dce6d_level0_col2\" class=\"col_heading level0 col2\" >context</th>\n",
       "      <th id=\"T_dce6d_level0_col3\" class=\"col_heading level0 col3\" >batch</th>\n",
       "      <th id=\"T_dce6d_level0_col4\" class=\"col_heading level0 col4\" >pos</th>\n",
       "      <th id=\"T_dce6d_level0_col5\" class=\"col_heading level0 col5\" >label</th>\n",
       "      <th id=\"T_dce6d_level0_col6\" class=\"col_heading level0 col6\" >feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row0\" class=\"row_heading level0 row0\" >117396</th>\n",
       "      <td id=\"T_dce6d_row0_col0\" class=\"data row0 col0\" >·technique</td>\n",
       "      <td id=\"T_dce6d_row0_col1\" class=\"data row0 col1\" >·technique/20</td>\n",
       "      <td id=\"T_dce6d_row0_col2\" class=\"data row0 col2\" >packet·sniffing·type·of|·technique|·has</td>\n",
       "      <td id=\"T_dce6d_row0_col3\" class=\"data row0 col3\" >917</td>\n",
       "      <td id=\"T_dce6d_row0_col4\" class=\"data row0 col4\" >20</td>\n",
       "      <td id=\"T_dce6d_row0_col5\" class=\"data row0 col5\" >917/20</td>\n",
       "      <td id=\"T_dce6d_row0_col6\" class=\"data row0 col6\" >1.536394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row1\" class=\"row_heading level0 row1\" >67267</th>\n",
       "      <td id=\"T_dce6d_row1_col0\" class=\"data row1 col0\" >·size</td>\n",
       "      <td id=\"T_dce6d_row1_col1\" class=\"data row1 col1\" >·size/67</td>\n",
       "      <td id=\"T_dce6d_row1_col2\" class=\"data row1 col2\" >·Returns·the·filter·kernel·of|·size|·(</td>\n",
       "      <td id=\"T_dce6d_row1_col3\" class=\"data row1 col3\" >525</td>\n",
       "      <td id=\"T_dce6d_row1_col4\" class=\"data row1 col4\" >67</td>\n",
       "      <td id=\"T_dce6d_row1_col5\" class=\"data row1 col5\" >525/67</td>\n",
       "      <td id=\"T_dce6d_row1_col6\" class=\"data row1 col6\" >1.530362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row2\" class=\"row_heading level0 row2\" >82757</th>\n",
       "      <td id=\"T_dce6d_row2_col0\" class=\"data row2 col0\" >·size</td>\n",
       "      <td id=\"T_dce6d_row2_col1\" class=\"data row2 col1\" >·size/69</td>\n",
       "      <td id=\"T_dce6d_row2_col2\" class=\"data row2 col2\" >·can·be·a·vector·of|·size|·one</td>\n",
       "      <td id=\"T_dce6d_row2_col3\" class=\"data row2 col3\" >646</td>\n",
       "      <td id=\"T_dce6d_row2_col4\" class=\"data row2 col4\" >69</td>\n",
       "      <td id=\"T_dce6d_row2_col5\" class=\"data row2 col5\" >646/69</td>\n",
       "      <td id=\"T_dce6d_row2_col6\" class=\"data row2 col6\" >1.464729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row3\" class=\"row_heading level0 row3\" >143900</th>\n",
       "      <td id=\"T_dce6d_row3_col0\" class=\"data row3 col0\" >·shape</td>\n",
       "      <td id=\"T_dce6d_row3_col1\" class=\"data row3 col1\" >·shape/28</td>\n",
       "      <td id=\"T_dce6d_row3_col2\" class=\"data row3 col2\" >:`ndarray`·of|·shape|·`</td>\n",
       "      <td id=\"T_dce6d_row3_col3\" class=\"data row3 col3\" >1124</td>\n",
       "      <td id=\"T_dce6d_row3_col4\" class=\"data row3 col4\" >28</td>\n",
       "      <td id=\"T_dce6d_row3_col5\" class=\"data row3 col5\" >1124/28</td>\n",
       "      <td id=\"T_dce6d_row3_col6\" class=\"data row3 col6\" >1.374878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row4\" class=\"row_heading level0 row4\" >8054</th>\n",
       "      <td id=\"T_dce6d_row4_col0\" class=\"data row4 col0\" >·culture</td>\n",
       "      <td id=\"T_dce6d_row4_col1\" class=\"data row4 col1\" >·culture/118</td>\n",
       "      <td id=\"T_dce6d_row4_col2\" class=\"data row4 col2\" >am·stressed·the·importance·of|·culture|·in</td>\n",
       "      <td id=\"T_dce6d_row4_col3\" class=\"data row4 col3\" >62</td>\n",
       "      <td id=\"T_dce6d_row4_col4\" class=\"data row4 col4\" >118</td>\n",
       "      <td id=\"T_dce6d_row4_col5\" class=\"data row4 col5\" >62/118</td>\n",
       "      <td id=\"T_dce6d_row4_col6\" class=\"data row4 col6\" >1.318117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row5\" class=\"row_heading level0 row5\" >6557</th>\n",
       "      <td id=\"T_dce6d_row5_col0\" class=\"data row5 col0\" >·culture</td>\n",
       "      <td id=\"T_dce6d_row5_col1\" class=\"data row5 col1\" >·culture/29</td>\n",
       "      <td id=\"T_dce6d_row5_col2\" class=\"data row5 col2\" >’s·the·hub·of|·culture|,</td>\n",
       "      <td id=\"T_dce6d_row5_col3\" class=\"data row5 col3\" >51</td>\n",
       "      <td id=\"T_dce6d_row5_col4\" class=\"data row5 col4\" >29</td>\n",
       "      <td id=\"T_dce6d_row5_col5\" class=\"data row5 col5\" >51/29</td>\n",
       "      <td id=\"T_dce6d_row5_col6\" class=\"data row5 col6\" >1.271067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row6\" class=\"row_heading level0 row6\" >158835</th>\n",
       "      <td id=\"T_dce6d_row6_col0\" class=\"data row6 col0\" >·bike</td>\n",
       "      <td id=\"T_dce6d_row6_col1\" class=\"data row6 col1\" >·bike/115</td>\n",
       "      <td id=\"T_dce6d_row6_col2\" class=\"data row6 col2\" >·smells·of·the·rubber·of|·bike|·tires</td>\n",
       "      <td id=\"T_dce6d_row6_col3\" class=\"data row6 col3\" >1240</td>\n",
       "      <td id=\"T_dce6d_row6_col4\" class=\"data row6 col4\" >115</td>\n",
       "      <td id=\"T_dce6d_row6_col5\" class=\"data row6 col5\" >1240/115</td>\n",
       "      <td id=\"T_dce6d_row6_col6\" class=\"data row6 col6\" >1.250245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row7\" class=\"row_heading level0 row7\" >105191</th>\n",
       "      <td id=\"T_dce6d_row7_col0\" class=\"data row7 col0\" >·flavor</td>\n",
       "      <td id=\"T_dce6d_row7_col1\" class=\"data row7 col1\" >·flavor/103</td>\n",
       "      <td id=\"T_dce6d_row7_col2\" class=\"data row7 col2\" >·its’·massive·explosion·of|·flavor|!</td>\n",
       "      <td id=\"T_dce6d_row7_col3\" class=\"data row7 col3\" >821</td>\n",
       "      <td id=\"T_dce6d_row7_col4\" class=\"data row7 col4\" >103</td>\n",
       "      <td id=\"T_dce6d_row7_col5\" class=\"data row7 col5\" >821/103</td>\n",
       "      <td id=\"T_dce6d_row7_col6\" class=\"data row7 col6\" >1.244410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row8\" class=\"row_heading level0 row8\" >82891</th>\n",
       "      <td id=\"T_dce6d_row8_col0\" class=\"data row8 col0\" >·cooperative</td>\n",
       "      <td id=\"T_dce6d_row8_col1\" class=\"data row8 col1\" >·cooperative/75</td>\n",
       "      <td id=\"T_dce6d_row8_col2\" class=\"data row8 col2\" >·management·and·rollout·of|·cooperative|·marketing</td>\n",
       "      <td id=\"T_dce6d_row8_col3\" class=\"data row8 col3\" >647</td>\n",
       "      <td id=\"T_dce6d_row8_col4\" class=\"data row8 col4\" >75</td>\n",
       "      <td id=\"T_dce6d_row8_col5\" class=\"data row8 col5\" >647/75</td>\n",
       "      <td id=\"T_dce6d_row8_col6\" class=\"data row8 col6\" >1.208105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row9\" class=\"row_heading level0 row9\" >96031</th>\n",
       "      <td id=\"T_dce6d_row9_col0\" class=\"data row9 col0\" >·dimension</td>\n",
       "      <td id=\"T_dce6d_row9_col1\" class=\"data row9 col1\" >·dimension/31</td>\n",
       "      <td id=\"T_dce6d_row9_col2\" class=\"data row9 col2\" >nSparse·histogram·of|·dimension|·2</td>\n",
       "      <td id=\"T_dce6d_row9_col3\" class=\"data row9 col3\" >750</td>\n",
       "      <td id=\"T_dce6d_row9_col4\" class=\"data row9 col4\" >31</td>\n",
       "      <td id=\"T_dce6d_row9_col5\" class=\"data row9 col5\" >750/31</td>\n",
       "      <td id=\"T_dce6d_row9_col6\" class=\"data row9 col6\" >1.205151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row10\" class=\"row_heading level0 row10\" >100347</th>\n",
       "      <td id=\"T_dce6d_row10_col0\" class=\"data row10 col0\" >·productivity</td>\n",
       "      <td id=\"T_dce6d_row10_col1\" class=\"data row10 col1\" >·productivity/123</td>\n",
       "      <td id=\"T_dce6d_row10_col2\" class=\"data row10 col2\" >·replaced.↩\n",
       "Loss·of|·productivity|·of</td>\n",
       "      <td id=\"T_dce6d_row10_col3\" class=\"data row10 col3\" >783</td>\n",
       "      <td id=\"T_dce6d_row10_col4\" class=\"data row10 col4\" >123</td>\n",
       "      <td id=\"T_dce6d_row10_col5\" class=\"data row10 col5\" >783/123</td>\n",
       "      <td id=\"T_dce6d_row10_col6\" class=\"data row10 col6\" >1.201225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row11\" class=\"row_heading level0 row11\" >30952</th>\n",
       "      <td id=\"T_dce6d_row11_col0\" class=\"data row11 col0\" >·platform</td>\n",
       "      <td id=\"T_dce6d_row11_col1\" class=\"data row11 col1\" >·platform/104</td>\n",
       "      <td id=\"T_dce6d_row11_col2\" class=\"data row11 col2\" >·prior·to·the·removal·of|·platform|.</td>\n",
       "      <td id=\"T_dce6d_row11_col3\" class=\"data row11 col3\" >241</td>\n",
       "      <td id=\"T_dce6d_row11_col4\" class=\"data row11 col4\" >104</td>\n",
       "      <td id=\"T_dce6d_row11_col5\" class=\"data row11 col5\" >241/104</td>\n",
       "      <td id=\"T_dce6d_row11_col6\" class=\"data row11 col6\" >1.179424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row12\" class=\"row_heading level0 row12\" >86572</th>\n",
       "      <td id=\"T_dce6d_row12_col0\" class=\"data row12 col0\" >·body</td>\n",
       "      <td id=\"T_dce6d_row12_col1\" class=\"data row12 col1\" >·body/44</td>\n",
       "      <td id=\"T_dce6d_row12_col2\" class=\"data row12 col2\" >#·MD5·hash·of|·body|↩\n",
       "</td>\n",
       "      <td id=\"T_dce6d_row12_col3\" class=\"data row12 col3\" >676</td>\n",
       "      <td id=\"T_dce6d_row12_col4\" class=\"data row12 col4\" >44</td>\n",
       "      <td id=\"T_dce6d_row12_col5\" class=\"data row12 col5\" >676/44</td>\n",
       "      <td id=\"T_dce6d_row12_col6\" class=\"data row12 col6\" >1.166018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row13\" class=\"row_heading level0 row13\" >154019</th>\n",
       "      <td id=\"T_dce6d_row13_col0\" class=\"data row13 col0\" >·creation</td>\n",
       "      <td id=\"T_dce6d_row13_col1\" class=\"data row13 col1\" >·creation/35</td>\n",
       "      <td id=\"T_dce6d_row13_col2\" class=\"data row13 col2\" >·of·the·true·nature·of|·creation|,</td>\n",
       "      <td id=\"T_dce6d_row13_col3\" class=\"data row13 col3\" >1203</td>\n",
       "      <td id=\"T_dce6d_row13_col4\" class=\"data row13 col4\" >35</td>\n",
       "      <td id=\"T_dce6d_row13_col5\" class=\"data row13 col5\" >1203/35</td>\n",
       "      <td id=\"T_dce6d_row13_col6\" class=\"data row13 col6\" >1.165724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row14\" class=\"row_heading level0 row14\" >55622</th>\n",
       "      <td id=\"T_dce6d_row14_col0\" class=\"data row14 col0\" >·device</td>\n",
       "      <td id=\"T_dce6d_row14_col1\" class=\"data row14 col1\" >·device/70</td>\n",
       "      <td id=\"T_dce6d_row14_col2\" class=\"data row14 col2\" >·supply·list↩\n",
       "········#·of|·device|·prefix</td>\n",
       "      <td id=\"T_dce6d_row14_col3\" class=\"data row14 col3\" >434</td>\n",
       "      <td id=\"T_dce6d_row14_col4\" class=\"data row14 col4\" >70</td>\n",
       "      <td id=\"T_dce6d_row14_col5\" class=\"data row14 col5\" >434/70</td>\n",
       "      <td id=\"T_dce6d_row14_col6\" class=\"data row14 col6\" >1.149153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row15\" class=\"row_heading level0 row15\" >151554</th>\n",
       "      <td id=\"T_dce6d_row15_col0\" class=\"data row15 col0\" >·literacy</td>\n",
       "      <td id=\"T_dce6d_row15_col1\" class=\"data row15 col1\" >·literacy/2</td>\n",
       "      <td id=\"T_dce6d_row15_col2\" class=\"data row15 col2\" ><|BOS|>·of|·literacy|.</td>\n",
       "      <td id=\"T_dce6d_row15_col3\" class=\"data row15 col3\" >1184</td>\n",
       "      <td id=\"T_dce6d_row15_col4\" class=\"data row15 col4\" >2</td>\n",
       "      <td id=\"T_dce6d_row15_col5\" class=\"data row15 col5\" >1184/2</td>\n",
       "      <td id=\"T_dce6d_row15_col6\" class=\"data row15 col6\" >1.127405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row16\" class=\"row_heading level0 row16\" >151568</th>\n",
       "      <td id=\"T_dce6d_row16_col0\" class=\"data row16 col0\" >·input</td>\n",
       "      <td id=\"T_dce6d_row16_col1\" class=\"data row16 col1\" >·input/16</td>\n",
       "      <td id=\"T_dce6d_row16_col2\" class=\"data row16 col2\" >·offer·many·simple·ways·of|·input|ting</td>\n",
       "      <td id=\"T_dce6d_row16_col3\" class=\"data row16 col3\" >1184</td>\n",
       "      <td id=\"T_dce6d_row16_col4\" class=\"data row16 col4\" >16</td>\n",
       "      <td id=\"T_dce6d_row16_col5\" class=\"data row16 col5\" >1184/16</td>\n",
       "      <td id=\"T_dce6d_row16_col6\" class=\"data row16 col6\" >1.093716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row17\" class=\"row_heading level0 row17\" >153553</th>\n",
       "      <td id=\"T_dce6d_row17_col0\" class=\"data row17 col0\" >·resource</td>\n",
       "      <td id=\"T_dce6d_row17_col1\" class=\"data row17 col1\" >·resource/81</td>\n",
       "      <td id=\"T_dce6d_row17_col2\" class=\"data row17 col2\" >except·for·url)·of|·resource|·is</td>\n",
       "      <td id=\"T_dce6d_row17_col3\" class=\"data row17 col3\" >1199</td>\n",
       "      <td id=\"T_dce6d_row17_col4\" class=\"data row17 col4\" >81</td>\n",
       "      <td id=\"T_dce6d_row17_col5\" class=\"data row17 col5\" >1199/81</td>\n",
       "      <td id=\"T_dce6d_row17_col6\" class=\"data row17 col6\" >1.090575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row18\" class=\"row_heading level0 row18\" >7407</th>\n",
       "      <td id=\"T_dce6d_row18_col0\" class=\"data row18 col0\" >·marketing</td>\n",
       "      <td id=\"T_dce6d_row18_col1\" class=\"data row18 col1\" >·marketing/111</td>\n",
       "      <td id=\"T_dce6d_row18_col2\" class=\"data row18 col2\" >·with·20·years·of|·marketing|·and</td>\n",
       "      <td id=\"T_dce6d_row18_col3\" class=\"data row18 col3\" >57</td>\n",
       "      <td id=\"T_dce6d_row18_col4\" class=\"data row18 col4\" >111</td>\n",
       "      <td id=\"T_dce6d_row18_col5\" class=\"data row18 col5\" >57/111</td>\n",
       "      <td id=\"T_dce6d_row18_col6\" class=\"data row18 col6\" >1.089262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dce6d_level0_row19\" class=\"row_heading level0 row19\" >152157</th>\n",
       "      <td id=\"T_dce6d_row19_col0\" class=\"data row19 col0\" >·file</td>\n",
       "      <td id=\"T_dce6d_row19_col1\" class=\"data row19 col1\" >·file/93</td>\n",
       "      <td id=\"T_dce6d_row19_col2\" class=\"data row19 col2\" >\"↩\n",
       "#·list·of|·file|-</td>\n",
       "      <td id=\"T_dce6d_row19_col3\" class=\"data row19 col3\" >1188</td>\n",
       "      <td id=\"T_dce6d_row19_col4\" class=\"data row19 col4\" >93</td>\n",
       "      <td id=\"T_dce6d_row19_col5\" class=\"data row19 col5\" >1188/93</td>\n",
       "      <td id=\"T_dce6d_row19_col6\" class=\"data row19 col6\" >1.085569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7ef95eb9d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df = make_token_df(tokens)\n",
    "token_df[\"feature\"] = utils.to_numpy(hidden_acts[:, feature_id])\n",
    "token_df.sort_values(\"feature\", ascending=False).head(20).style.background_gradient(\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit weights: To understand what logits the SAE features boosts the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_798c4_row0_col1, #T_798c4_row1_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_798c4_row2_col1 {\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_798c4_row3_col1 {\n",
       "  background-color: #e6d7cf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_798c4_row4_col1 {\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_798c4_row5_col1 {\n",
       "  background-color: #a3c2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_798c4_row6_col1 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_798c4_row7_col1 {\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_798c4_row8_col1 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_798c4_row9_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_798c4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_798c4_level0_col0\" class=\"col_heading level0 col0\" >token</th>\n",
       "      <th id=\"T_798c4_level0_col1\" class=\"col_heading level0 col1\" >logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row0\" class=\"row_heading level0 row0\" >3876</th>\n",
       "      <td id=\"T_798c4_row0_col0\" class=\"data row0 col0\" >lation</td>\n",
       "      <td id=\"T_798c4_row0_col1\" class=\"data row0 col1\" >1.497491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row1\" class=\"row_heading level0 row1\" >23030</th>\n",
       "      <td id=\"T_798c4_row1_col0\" class=\"data row1 col0\" >lessness</td>\n",
       "      <td id=\"T_798c4_row1_col1\" class=\"data row1 col1\" >1.497241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row2\" class=\"row_heading level0 row2\" >46767</th>\n",
       "      <td id=\"T_798c4_row2_col0\" class=\"data row2 col0\" >·awaited</td>\n",
       "      <td id=\"T_798c4_row2_col1\" class=\"data row2 col1\" >1.401943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row3\" class=\"row_heading level0 row3\" >2961</th>\n",
       "      <td id=\"T_798c4_row3_col0\" class=\"data row3 col0\" >·needed</td>\n",
       "      <td id=\"T_798c4_row3_col1\" class=\"data row3 col1\" >1.365549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row4\" class=\"row_heading level0 row4\" >45470</th>\n",
       "      <td id=\"T_798c4_row4_col0\" class=\"data row4 col0\" >reth</td>\n",
       "      <td id=\"T_798c4_row4_col1\" class=\"data row4 col1\" >1.307851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row5\" class=\"row_heading level0 row5\" >21571</th>\n",
       "      <td id=\"T_798c4_row5_col0\" class=\"data row5 col0\" >heid</td>\n",
       "      <td id=\"T_798c4_row5_col1\" class=\"data row5 col1\" >1.301885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row6\" class=\"row_heading level0 row6\" >31519</th>\n",
       "      <td id=\"T_798c4_row6_col0\" class=\"data row6 col0\" >·borne</td>\n",
       "      <td id=\"T_798c4_row6_col1\" class=\"data row6 col1\" >1.274774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row7\" class=\"row_heading level0 row7\" >44266</th>\n",
       "      <td id=\"T_798c4_row7_col0\" class=\"data row7 col0\" >ickness</td>\n",
       "      <td id=\"T_798c4_row7_col1\" class=\"data row7 col1\" >1.236776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row8\" class=\"row_heading level0 row8\" >5375</th>\n",
       "      <td id=\"T_798c4_row8_col0\" class=\"data row8 col0\" >encing</td>\n",
       "      <td id=\"T_798c4_row8_col1\" class=\"data row8 col1\" >1.221099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_798c4_level0_row9\" class=\"row_heading level0 row9\" >1295</th>\n",
       "      <td id=\"T_798c4_row9_col0\" class=\"data row9 col0\" >ality</td>\n",
       "      <td id=\"T_798c4_row9_col1\" class=\"data row9 col1\" >1.211532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f807825f8b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_effect = encoder.W_dec[feature_id] @ model.W_out[0] @ model.W_U\n",
    "create_vocab_df(logit_effect).head(10).style.background_gradient(\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "direct path and de-embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tried using backward hooks but need some time to debug to understand how to setup the intermediate activations for the backward pass. Will use quick hack for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = model.to_string(tokens[1088, :56])\n",
    "token_example = model.to_tokens(s)\n",
    "logits, cache = model.run_with_cache(token_example, stop_at_layer=1)\n",
    "print(s)\n",
    "\n",
    "w_enc = encoder.W_enc[:, feature_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried to use backward hooks but need to spend time to debug the issue here. Will use the quick hack for now.\n",
    "\n",
    "# model.set_use_attn_result(True)\n",
    "# model.set_use_attn_in(True)\n",
    "# model.set_use_hook_mlp_in(True)\n",
    "\n",
    "# filter_not_qkv_input = lambda name: \"_input\" not in name\n",
    "# model.reset_hooks()\n",
    "# cache = {}\n",
    "# def forward_cache_hook(act, hook):\n",
    "#     cache[hook.name] = act.detach()\n",
    "# model.add_hook(filter_not_qkv_input, forward_cache_hook, \"fwd\")\n",
    "\n",
    "# grad_cache = {}\n",
    "# def backward_cache_hook(act, hook):\n",
    "#     grad_cache[hook.name] = act.detach()\n",
    "# model.add_hook(filter_not_qkv_input, backward_cache_hook, \"bwd\")\n",
    "\n",
    "# logits = model(token_example)\n",
    "\n",
    "# cache['blocks.0.mlp.hook_post'].requires_grad_(True)\n",
    "# x_mlpout = cache['blocks.0.mlp.hook_post'][0, -1]\n",
    "# feature_act = einops.einsum(x_mlpout, w_enc, \"hdim, hdim -> \")\n",
    "# feature_act.backward()\n",
    "\n",
    "# these are the cached activations of the model\n",
    "# fwd_cache = ActivationCache(cache, model)\n",
    "\n",
    "# do the backward pass the on SAE feature activations\n",
    "# x_mlpout = cache['blocks.0.mlp.hook_post'][0, -1]\n",
    "# w_enc = encoder.W_enc[:, feature_id]\n",
    "# feature_act = einops.einsum(x_mlpout, w_enc, \"hdim, hdim -> \")\n",
    "# feature_act.backward()\n",
    "\n",
    "# bwd_cache = ActivationCache(grad_cache, model)\n",
    "\n",
    "# model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick hack using a seprate MLP model for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (act): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell just once, don't need to reload the MLP for each feature\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc = nn.Linear(model.cfg.d_model ,model.cfg.d_mlp)\n",
    "        self.act = torch.nn.GELU()\n",
    "\n",
    "        #? Initialize the linear layer with the provided weights and biases\n",
    "        self.fc.weight.data = model.W_in[0].T.detach()\n",
    "        self.fc.bias.data = model.b_in[0].detach()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.fc(x))\n",
    "\n",
    "mlp_model = MLP()\n",
    "mlp_model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the POPvX. This 'data' file swapping/packet sniffing type of technique\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    s = model.to_string(all_tokens[917, 1:21])\n",
    "    token_example = model.to_tokens(s)\n",
    "    logits, cache = model.run_with_cache(token_example, stop_at_layer=1)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   276,   254, 41261,    88,    58,    16,   826,   684,  2137,\n",
       "             9,  1819,  1810,  5276,    17, 27581, 30367,   273,  1479,   274,\n",
       "          5686]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1, 6231,  625]], device='cuda:0')\n",
      "['<|BOS|>', ' aren', \"'t\"]\n"
     ]
    }
   ],
   "source": [
    "print(model.to_tokens(\" aren't\"))\n",
    "print(model.to_str_tokens(\" aren't\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  torch.Size([512])\n",
      "MLP Output Shape:  torch.Size([2048])\n",
      "Feature Activation:  tensor(1.9229, device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "Actual Feature Activation:  tensor(1.9229, device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is the input x to the MLP Layer \n",
    "x_mid = torch.autograd.Variable(cache['blocks.0.ln2.hook_normalized'][0, -1], requires_grad=True)\n",
    "print(\"Input Shape: \", x_mid.shape)\n",
    "\n",
    "# encoder feature direction \n",
    "w_enc = encoder.W_enc[:, feature_id]\n",
    "\n",
    "x_mlpout = mlp_model(x_mid)\n",
    "print(\"MLP Output Shape: \", x_mlpout.shape)\n",
    "\n",
    "# print(\"The post MLP activations match well: \", torch.abs(cache['blocks.0.mlp.hook_post'][0, -1] - x_mlpout).max().item())\n",
    "feature_act = einops.einsum(x_mlpout, w_enc, \"hdim, hdim -> \")\n",
    "actual_feature_act = einops.einsum(cache['blocks.0.mlp.hook_post'][0, -1], w_enc, \"hdim, hdim -> \")\n",
    "print(\"Feature Activation: \", feature_act)\n",
    "print(\"Actual Feature Activation: \", actual_feature_act)\n",
    "\n",
    "# Do the backward pass on the activation to get the linear approximation\n",
    "feature_act.backward()\n",
    "\n",
    "#! this is the feature vector in MLP input space\n",
    "n_mid = x_mid.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an approximation for how much each token in the model's vocabulary contributes to activating the original SAE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_score_df(score_vec, make_probs=False, full_vocab=None, ascending=False):\n",
    "    full_vocab = process_tokens(model.to_str_tokens(torch.arange(model.cfg.d_vocab)))\n",
    "    vocab_df = pd.DataFrame({\"token\": full_vocab, \"feature_scores\": utils.to_numpy(score_vec)})\n",
    "    return vocab_df.sort_values(\"feature_scores\", ascending=ascending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De-embedding token scores for the direct path to the linearized SAE feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_619bf_row0_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_619bf_row1_col1 {\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_619bf_row2_col1 {\n",
       "  background-color: #e67259;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_619bf_row3_col1 {\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_619bf_row4_col1 {\n",
       "  background-color: #f0cdbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_619bf_row5_col1 {\n",
       "  background-color: #e4d9d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_619bf_row6_col1 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_619bf_row7_col1 {\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_619bf_row8_col1 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_619bf_row9_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_619bf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_619bf_level0_col0\" class=\"col_heading level0 col0\" >token</th>\n",
       "      <th id=\"T_619bf_level0_col1\" class=\"col_heading level0 col1\" >feature_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row0\" class=\"row_heading level0 row0\" >44535</th>\n",
       "      <td id=\"T_619bf_row0_col0\" class=\"data row0 col0\" >·pleural</td>\n",
       "      <td id=\"T_619bf_row0_col1\" class=\"data row0 col1\" >0.330833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row1\" class=\"row_heading level0 row1\" >31259</th>\n",
       "      <td id=\"T_619bf_row1_col0\" class=\"data row1 col0\" >·McN</td>\n",
       "      <td id=\"T_619bf_row1_col1\" class=\"data row1 col1\" >0.330375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row2\" class=\"row_heading level0 row2\" >16029</th>\n",
       "      <td id=\"T_619bf_row2_col0\" class=\"data row2 col0\" >�</td>\n",
       "      <td id=\"T_619bf_row2_col1\" class=\"data row2 col1\" >0.326469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row3\" class=\"row_heading level0 row3\" >9784</th>\n",
       "      <td id=\"T_619bf_row3_col0\" class=\"data row3 col0\" >·Administ</td>\n",
       "      <td id=\"T_619bf_row3_col1\" class=\"data row3 col1\" >0.321786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row4\" class=\"row_heading level0 row4\" >1666</th>\n",
       "      <td id=\"T_619bf_row4_col0\" class=\"data row4 col0\" >·associ</td>\n",
       "      <td id=\"T_619bf_row4_col1\" class=\"data row4 col1\" >0.319722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row5\" class=\"row_heading level0 row5\" >34144</th>\n",
       "      <td id=\"T_619bf_row5_col0\" class=\"data row5 col0\" >·Tort</td>\n",
       "      <td id=\"T_619bf_row5_col1\" class=\"data row5 col1\" >0.318106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row6\" class=\"row_heading level0 row6\" >17445</th>\n",
       "      <td id=\"T_619bf_row6_col0\" class=\"data row6 col0\" >·Iss</td>\n",
       "      <td id=\"T_619bf_row6_col1\" class=\"data row6 col1\" >0.310321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row7\" class=\"row_heading level0 row7\" >4238</th>\n",
       "      <td id=\"T_619bf_row7_col0\" class=\"data row7 col0\" >·Mich</td>\n",
       "      <td id=\"T_619bf_row7_col1\" class=\"data row7 col1\" >0.309697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row8\" class=\"row_heading level0 row8\" >27281</th>\n",
       "      <td id=\"T_619bf_row8_col0\" class=\"data row8 col0\" >·ż</td>\n",
       "      <td id=\"T_619bf_row8_col1\" class=\"data row8 col1\" >0.306936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_619bf_level0_row9\" class=\"row_heading level0 row9\" >43179</th>\n",
       "      <td id=\"T_619bf_row9_col0\" class=\"data row9 col0\" >·noct</td>\n",
       "      <td id=\"T_619bf_row9_col1\" class=\"data row9 col1\" >0.303841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7ef8505ba0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# direct path -> through de-embedding we can analyze how much each token contributes to the feature in the MLP input space\n",
    "direct_path_scores = einops.einsum(model.W_E, n_mid, \"n_vocab dim, dim -> n_vocab\")\n",
    "feature_score_df(direct_path_scores).head(10).style.background_gradient(\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention: Analyzing the OV Circuit and the QK Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the POPvX. This 'data' file swapping/packet sniffing type of technique\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern shape:  torch.Size([8, 21, 21])\n"
     ]
    }
   ],
   "source": [
    "str_tokens = model.to_str_tokens(s)\n",
    "\n",
    "patterns = cache['blocks.0.attn.hook_pattern'][0] # attention pattern for the layer0 \n",
    "print(\"attention pattern shape: \", patterns.shape)\n",
    "labels = [str(i) for i in range(8)]\n",
    "\n",
    "plot = attention_heads(attention=patterns, tokens=str_tokens, attention_head_names=labels).show_code()\n",
    "\n",
    "# Display the title\n",
    "title = \"Attention Pattern Analysis\"\n",
    "title_html = f\"<h2>{title}</h2><br/>\"\n",
    "\n",
    "# Return the visualisation as raw code\n",
    "html = f\"<div style='max-width: {str(700)}px;'>{title_html + plot}</div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|BOS|>', ' in', ' the', ' POP', 'v', 'X', '.', ' This', \" '\", 'data', \"'\", ' file', ' sw', 'apping', '/', 'packet', ' sniff', 'ing', ' type', ' of', ' technique']\n"
     ]
    }
   ],
   "source": [
    "print(str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='max-width: 700px;'><h2>Attention Pattern Analysis</h2><br/><div id=\"circuits-vis-f8682f67-09ce\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-f8682f67-09ce\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9099342226982117, 0.09006579220294952, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5092722773551941, 0.45896419882774353, 0.03176351264119148, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8369204998016357, 0.037019167095422745, 0.06872885674238205, 0.05733151361346245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39499756693840027, 0.023899592459201813, 0.03536667302250862, 0.505749523639679, 0.039986658841371536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.625881016254425, 0.021038182079792023, 0.021693414077162743, 0.1103825643658638, 0.08620991557836533, 0.1347949355840683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48876243829727173, 0.0037938752211630344, 0.002215175423771143, 0.01657126657664776, 0.09449098259210587, 0.25687262415885925, 0.13729359209537506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6170348525047302, 0.0017004129476845264, 0.0009057584684342146, 0.003305853111669421, 0.002267837757244706, 0.0070558469742536545, 0.25890955328941345, 0.10881993174552917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4499516189098358, 0.0007681469433009624, 0.0007622498669661582, 0.002269947435706854, 0.0064350818283855915, 0.005526742897927761, 0.13820752501487732, 0.25288522243499756, 0.14319349825382233, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18225404620170593, 0.0004993727197870612, 0.0002641730825416744, 0.0003973380953539163, 0.0010058293119072914, 0.008526607416570187, 0.04376308247447014, 0.16955728828907013, 0.5269734263420105, 0.06675887107849121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5945902466773987, 0.00034538577892817557, 0.0001622888376004994, 0.00034660499659366906, 0.0012701154919341207, 0.004843628965318203, 0.011049733497202396, 0.030047593638300896, 0.07376953959465027, 0.22439858317375183, 0.0591762438416481, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5918294787406921, 0.00021191548148635775, 6.255252083064988e-05, 7.080873183440417e-05, 5.272159614833072e-05, 0.0005740191554650664, 0.004697057418525219, 0.027063652873039246, 0.025316977873444557, 0.0344947874546051, 0.20965534448623657, 0.10597069561481476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8198919296264648, 0.00026428521960042417, 0.00012075063568772748, 6.990043766563758e-05, 5.0894457672256976e-05, 8.597753912908956e-05, 0.0016224038554355502, 0.0051368060521781445, 0.0042548212222754955, 0.01725432462990284, 0.018013449385762215, 0.11123190820217133, 0.0220024436712265, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3186247944831848, 0.0002478492388036102, 0.00035631199716590345, 0.00022450252436101437, 3.4758864785544574e-05, 9.816534293349832e-05, 0.0016502864891663194, 0.0055282870307564735, 0.014573161490261555, 0.023833459243178368, 0.10376603901386261, 0.19184406101703644, 0.19941408932209015, 0.1398041993379593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5878400206565857, 0.00010912596917478368, 7.845953950891271e-05, 0.0016307412879541516, 0.0002596497943159193, 0.00014943660062272102, 0.00014995723904576153, 0.0005219511804170907, 0.00327436625957489, 0.004865322262048721, 0.018777651712298393, 0.0462866872549057, 0.05851104483008385, 0.21902982890605927, 0.058515746146440506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3210994303226471, 3.37664969265461e-05, 3.625484896474518e-05, 0.00013661418051924556, 5.004405466024764e-05, 0.00013754353858530521, 0.00028299939003773034, 0.0007907983963377774, 0.003393564373254776, 0.0016250512562692165, 0.019795477390289307, 0.01089833490550518, 0.04971243068575859, 0.08285584300756454, 0.45949074625968933, 0.0496610626578331, 0.0, 0.0, 0.0, 0.0, 0.0], [0.477975457906723, 2.6863459424930625e-05, 3.96492141589988e-05, 6.223638774827123e-05, 0.00020822686201427132, 0.0001594353379914537, 0.00024375863722525537, 0.0005648658261634409, 0.0007808462833054364, 0.002636509947478771, 0.002744977129623294, 0.008325044065713882, 0.0066908798180520535, 0.021765688434243202, 0.03759140148758888, 0.25059783458709717, 0.18958640098571777, 0.0, 0.0, 0.0, 0.0], [0.16003091633319855, 7.582692887808662e-06, 1.1660979907901492e-05, 1.3416113688435871e-05, 1.0547917554504238e-05, 3.4290562325622886e-05, 0.0001908403355628252, 0.0004567984433379024, 0.0008131153299473226, 0.0007597687654197216, 0.0022815291304141283, 0.0030630745459347963, 0.004463748540729284, 0.004237278364598751, 0.027255218476057053, 0.05954324081540108, 0.6851761937141418, 0.05165080353617668, 0.0, 0.0, 0.0], [0.480042040348053, 6.504519114969298e-05, 4.1808078094618395e-05, 2.5318518964922987e-05, 2.6330610126024112e-05, 5.625977792078629e-05, 0.00016963330563157797, 0.0006045500631444156, 0.0006297384388744831, 0.00035835523158311844, 0.0006045604241080582, 0.0013349298387765884, 0.0034758877009153366, 0.004432137124240398, 0.016527710482478142, 0.03129541873931885, 0.05933060869574547, 0.2718351185321808, 0.1291445940732956, 0.0, 0.0], [0.32556676864624023, 0.00013331824447959661, 5.4039501264924183e-05, 2.492422026989516e-05, 3.201967956556473e-06, 6.688792382192332e-06, 2.537420004955493e-05, 6.957409641472623e-05, 0.00010652010678313673, 1.990781856875401e-05, 4.390749745653011e-05, 0.00020528676395770162, 0.0013621204998344183, 0.004384276922792196, 0.004384971223771572, 0.004756603855639696, 0.047882940620183945, 0.03752254322171211, 0.5532666444778442, 0.02018043026328087, 0.0], [0.543786346912384, 0.0005712254787795246, 0.0001917789486469701, 7.020379416644573e-05, 5.527192570298212e-06, 2.438116462144535e-05, 4.3464209738885984e-05, 5.38819185749162e-05, 6.809857586631551e-05, 2.9086108042974956e-05, 7.7005461207591e-05, 3.790640039369464e-05, 5.951559796812944e-05, 0.0002901182451751083, 0.0023586638271808624, 0.001010469044558704, 0.003878268413245678, 0.0060339136980473995, 0.13323406875133514, 0.22889630496501923, 0.07927977293729782]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9097885489463806, 0.0902114063501358, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8430934548377991, 0.08262107521295547, 0.07428544759750366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8417899012565613, 0.07310467213392258, 0.05966729298233986, 0.025438131764531136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6923162937164307, 0.13424009084701538, 0.09356909245252609, 0.06174471229314804, 0.018129870295524597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4588116705417633, 0.1960638165473938, 0.16255494952201843, 0.11556743830442429, 0.021606680005788803, 0.04539550840854645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4465412199497223, 0.12113005667924881, 0.0625283420085907, 0.07736112177371979, 0.029227834194898605, 0.0347595289349556, 0.2284519225358963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6767449975013733, 0.014852393418550491, 0.013048587366938591, 0.011116856709122658, 0.006639575120061636, 0.008317629806697369, 0.18514348566532135, 0.08413653075695038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3650650680065155, 0.03977876901626587, 0.017553068697452545, 0.029076598584651947, 0.01566079631447792, 0.018907897174358368, 0.15910017490386963, 0.18183551728725433, 0.17302216589450836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02957456186413765, 0.029476938769221306, 0.022258928045630455, 0.010358179919421673, 0.0017277752049267292, 0.006361663807183504, 0.13718196749687195, 0.23007573187351227, 0.5242292881011963, 0.008754945360124111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17809446156024933, 0.0456366240978241, 0.03843706473708153, 0.006994748953729868, 0.005317858420312405, 0.009214145131409168, 0.13884460926055908, 0.27881619334220886, 0.2041892409324646, 0.010988750495016575, 0.08346628397703171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07520462572574615, 0.01383123081177473, 0.011572450399398804, 0.005121962632983923, 0.0030301916413009167, 0.0033579235896468163, 0.09793577343225479, 0.14820288121700287, 0.28722697496414185, 0.011202624998986721, 0.3186127841472626, 0.02470051869750023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5463498830795288, 0.013343347236514091, 0.007799203507602215, 0.008202116936445236, 0.0018938278080895543, 0.005103007424622774, 0.047175388783216476, 0.055549055337905884, 0.10226982831954956, 0.01294616237282753, 0.13481980562210083, 0.04809096083045006, 0.016457486897706985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12456633150577545, 0.009800323285162449, 0.0075309015810489655, 0.004311942495405674, 0.0007118889479897916, 0.003347253892570734, 0.08883301168680191, 0.0697651281952858, 0.2115832269191742, 0.014976803213357925, 0.3390384912490845, 0.08787453174591064, 0.014816265553236008, 0.022844037041068077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30880820751190186, 0.0055222902446985245, 0.003376450389623642, 0.012246972881257534, 0.005853036418557167, 0.00427464721724391, 0.024832677096128464, 0.020036274567246437, 0.10796819627285004, 0.12668146193027496, 0.12866021692752838, 0.046777352690696716, 0.10273502767086029, 0.022727489471435547, 0.07949971407651901, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0294139813631773, 0.008473015390336514, 0.006392051000148058, 0.00453462777659297, 0.0005732434801757336, 0.0015156636945903301, 0.03715034946799278, 0.06203104928135872, 0.1589611917734146, 0.007175601087510586, 0.3145574629306793, 0.05993101745843887, 0.027963120490312576, 0.06749854981899261, 0.20271943509578705, 0.01110969576984644, 0.0, 0.0, 0.0, 0.0, 0.0], [0.254330039024353, 0.01009025052189827, 0.007023120764642954, 0.00393994664773345, 0.0007400836329907179, 0.0025722517166286707, 0.04504584148526192, 0.04769018292427063, 0.09964888542890549, 0.012589406222105026, 0.1274285465478897, 0.04733497276902199, 0.02002287283539772, 0.06800203025341034, 0.11851690709590912, 0.046598296612501144, 0.08842641115188599, 0.0, 0.0, 0.0, 0.0], [0.17544975876808167, 0.006733072455972433, 0.005086676217615604, 0.0036168419755995274, 0.000587267626542598, 0.0020667039789259434, 0.042797841131687164, 0.043349847197532654, 0.10828695446252823, 0.009784658439457417, 0.14358371496200562, 0.04610655456781387, 0.01976611651480198, 0.023411722853779793, 0.14492273330688477, 0.03544182330369949, 0.16554778814315796, 0.023459909483790398, 0.0, 0.0, 0.0], [0.15389184653759003, 0.007634020876139402, 0.0054600122384727, 0.002152525121346116, 0.0008941320702433586, 0.0016100319335237145, 0.040680646896362305, 0.04856385663151741, 0.10242265462875366, 0.00722278468310833, 0.15319162607192993, 0.01571355201303959, 0.01961507648229599, 0.04005969688296318, 0.07341758906841278, 0.017356377094984055, 0.11365059018135071, 0.046970002353191376, 0.14949297904968262, 0.0, 0.0], [0.2082594633102417, 0.0052465214394032955, 0.004236694425344467, 0.0024706635158509016, 0.0005881356191821396, 0.0013364596525207162, 0.022848883643746376, 0.01550342421978712, 0.022139502689242363, 0.007606683298945427, 0.022724350914359093, 0.044231899082660675, 0.01605175994336605, 0.029630547389388084, 0.018391231074929237, 0.06543352454900742, 0.12806583940982819, 0.04011332988739014, 0.2794668674468994, 0.06565417349338531, 0.0], [0.05094204843044281, 0.005481687840074301, 0.004194300156086683, 0.0017544860020279884, 0.0007936685578897595, 0.0008852762985043228, 0.03009084053337574, 0.03507578745484352, 0.046451859176158905, 0.0021993492264300585, 0.060464803129434586, 0.011964475736021996, 0.009144473820924759, 0.0208091139793396, 0.03177717700600624, 0.010648908093571663, 0.10217272490262985, 0.028671367093920708, 0.1988753378391266, 0.27517834305763245, 0.07242391258478165]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9465330243110657, 0.053466979414224625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9838224053382874, 0.012600181624293327, 0.0035774516873061657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9045770764350891, 0.01744934543967247, 0.006137999705970287, 0.07183560729026794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8251038193702698, 0.009429040364921093, 0.03573621064424515, 0.03948388621211052, 0.09024707227945328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7753403186798096, 0.009998563677072525, 0.003445131704211235, 0.0672868937253952, 0.09244824200868607, 0.05148085206747055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8807639479637146, 0.007518840488046408, 0.0038017190527170897, 0.03414244204759598, 0.024866042658686638, 0.022476542741060257, 0.026430459693074226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9109108448028564, 0.00936779286712408, 0.0022920342162251472, 0.02442479506134987, 0.008539658971130848, 0.011519181542098522, 0.009264757856726646, 0.023680882528424263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7995514869689941, 0.011643700301647186, 0.008289049379527569, 0.03953107073903084, 0.03530118614435196, 0.024476338177919388, 0.011524613946676254, 0.031177015975117683, 0.03850552439689636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6049643158912659, 0.016884298995137215, 0.003285246202722192, 0.03689821809530258, 0.01704336144030094, 0.009822659194469452, 0.017110329121351242, 0.01943334750831127, 0.11941646784543991, 0.15514174103736877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6435413360595703, 0.011472763493657112, 0.00869961827993393, 0.026368791237473488, 0.010845617391169071, 0.015315498225390911, 0.012815684080123901, 0.03137717768549919, 0.06805792450904846, 0.1327262669801712, 0.038779303431510925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49130377173423767, 0.011365469545125961, 0.011235061101615429, 0.01657511480152607, 0.007327640429139137, 0.00856893789023161, 0.008448870852589607, 0.02435719594359398, 0.11950612813234329, 0.17357176542282104, 0.06312675029039383, 0.06461334973573685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5215013027191162, 0.008006887510418892, 0.09715835750102997, 0.025521522387862206, 0.04208267107605934, 0.014060169458389282, 0.005399394314736128, 0.01120613981038332, 0.045079685747623444, 0.12427196651697159, 0.030103053897619247, 0.06510934978723526, 0.010499449446797371, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5426607131958008, 0.006699655205011368, 0.004627810325473547, 0.021665742620825768, 0.011751238256692886, 0.006295409519225359, 0.007398634217679501, 0.01710604317486286, 0.10626619309186935, 0.10761412233114243, 0.05876734480261803, 0.09717340767383575, 0.007682888302952051, 0.004290781915187836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5494278073310852, 0.009732076898217201, 0.023417454212903976, 0.03549071028828621, 0.024182317778468132, 0.015488369390368462, 0.00676616420969367, 0.010358990170061588, 0.04307049885392189, 0.17824316024780273, 0.028267666697502136, 0.05321967601776123, 0.007181922439485788, 0.010672003030776978, 0.004481127019971609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33696743845939636, 0.00822041742503643, 0.00519300764426589, 0.02068398892879486, 0.0108698895201087, 0.008535933680832386, 0.010631809011101723, 0.015654757618904114, 0.05863194912672043, 0.29544177651405334, 0.05034017190337181, 0.08220706135034561, 0.009454267099499702, 0.014455686323344707, 0.006116576958447695, 0.06659524887800217, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45618772506713867, 0.010100043378770351, 0.027502937242388725, 0.01802081987261772, 0.014068201184272766, 0.006512421183288097, 0.007424081210047007, 0.021650366485118866, 0.07482315599918365, 0.07527666538953781, 0.06403493136167526, 0.08530927449464798, 0.009976187720894814, 0.0074791512452065945, 0.00945932324975729, 0.07482995837926865, 0.03734477236866951, 0.0, 0.0, 0.0, 0.0], [0.4625812768936157, 0.009899868629872799, 0.0064682080410420895, 0.02459682710468769, 0.008459513075649738, 0.006309647113084793, 0.006893626879900694, 0.022314395755529404, 0.08368752896785736, 0.0862889215350151, 0.04826943948864937, 0.08650386333465576, 0.009991493076086044, 0.004924113862216473, 0.010532458312809467, 0.08677754551172256, 0.029257358983159065, 0.006243930198252201, 0.0, 0.0, 0.0], [0.3867228925228119, 0.010763335041701794, 0.006066018249839544, 0.0494452565908432, 0.012890786863863468, 0.006861887406557798, 0.006644388195127249, 0.022936953231692314, 0.09047827869653702, 0.12041333317756653, 0.03276193514466286, 0.08942849934101105, 0.008347527123987675, 0.006875351537019014, 0.0018969555385410786, 0.07519368082284927, 0.015583007596433163, 0.030156029388308525, 0.026533914729952812, 0.0, 0.0], [0.4879414141178131, 0.012406774796545506, 0.023878971114754677, 0.030511436983942986, 0.008568367920815945, 0.007169491145759821, 0.006229514256119728, 0.016560448333621025, 0.09553200006484985, 0.048590607941150665, 0.04341821372509003, 0.06642729043960571, 0.013384105637669563, 0.010811499319970608, 0.006309067830443382, 0.051484107971191406, 0.030219217762351036, 0.0073540667071938515, 0.027161085978150368, 0.006042321678251028, 0.0], [0.4120352864265442, 0.008927362971007824, 0.004046465270221233, 0.01148104015737772, 0.003756385063752532, 0.0066737518645823, 0.009505952708423138, 0.02370506152510643, 0.11992913484573364, 0.086598701775074, 0.05419304594397545, 0.07949849218130112, 0.009277082979679108, 0.008161243051290512, 0.013838481158018112, 0.05558233708143234, 0.027386877685785294, 0.004935439210385084, 0.03453699126839638, 0.005238235928118229, 0.020692745223641396]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9850037097930908, 0.014996319077908993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9819998741149902, 0.007133184466511011, 0.010867062956094742, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8694533705711365, 0.006968138739466667, 0.0018148598028346896, 0.12176360934972763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8213342428207397, 0.004193292930722237, 0.0015435739187523723, 0.005827870685607195, 0.16710098087787628, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.825353741645813, 0.012572353705763817, 0.0035205334424972534, 0.01173324789851904, 0.0034100301563739777, 0.14341020584106445, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9545570015907288, 0.015149668790400028, 0.015041935257613659, 0.006815658882260323, 0.0037226909771561623, 0.002325359731912613, 0.0023876160848885775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9453198909759521, 0.009954893961548805, 0.005507881287485361, 0.0023245541378855705, 0.0015041419537737966, 0.00039610802195966244, 0.00604731822386384, 0.028945114463567734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.895494282245636, 0.01323962863534689, 0.008555592969059944, 0.008157223463058472, 0.0039336043410003185, 0.005139194428920746, 0.012001261115074158, 0.014383697882294655, 0.039095547050237656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7515618801116943, 0.028307540342211723, 0.006618571002036333, 0.004689359106123447, 0.0008798525668680668, 0.0005574743263423443, 0.01929856650531292, 0.008245672099292278, 0.001456837053410709, 0.1783841997385025, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8405511379241943, 0.05853632465004921, 0.017507540062069893, 0.006664224434643984, 0.003520395839586854, 0.005621840711683035, 0.014677685685455799, 0.020127644762396812, 0.007669002283364534, 0.020521093159914017, 0.004603053443133831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8103798627853394, 0.04363364726305008, 0.01458690594881773, 0.003994878381490707, 0.0022451335098594427, 0.0009691047598607838, 0.017155323177576065, 0.01447464618831873, 0.0011902096448466182, 0.0069124940782785416, 0.004108589142560959, 0.08034925907850266, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7744272947311401, 0.0033273950684815645, 0.002118725562468171, 0.0005625227349810302, 0.003208000445738435, 0.0007769360672682524, 0.006245036609470844, 0.0013228700263425708, 0.001462487387470901, 0.0034581278450787067, 0.0007950132712721825, 0.001462107407860458, 0.2008334994316101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7984694838523865, 0.01442098617553711, 0.018322445452213287, 0.0005598511779680848, 0.0002692804846446961, 0.0002074377262033522, 0.01248747855424881, 0.019345102831721306, 0.0006630660500377417, 0.007060287520289421, 0.004680205602198839, 0.0031208288855850697, 0.0009849626803770661, 0.11940859258174896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8440337181091309, 0.009633923880755901, 0.008788224309682846, 0.009222556836903095, 0.009437362663447857, 0.022318165749311447, 0.014722349122166634, 0.008668472990393639, 0.005316199269145727, 0.04377400875091553, 0.010008938610553741, 0.008602031506597996, 0.0008568515768274665, 0.0009308364824391901, 0.0036864166613668203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.619860053062439, 0.03854535147547722, 0.013462495058774948, 0.02930387109518051, 0.0028959172777831554, 0.0018526904750615358, 0.03697885945439339, 0.016006765887141228, 0.0028660341631621122, 0.03175469487905502, 0.01683397777378559, 0.0035897246561944485, 0.0011555372038856149, 0.0038794009014964104, 0.012591375969350338, 0.16842326521873474, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8160427212715149, 0.0173658300191164, 0.011006535962224007, 0.005105454474687576, 0.0021517931018024683, 0.0002998752170242369, 0.015063932165503502, 0.010114075616002083, 0.0028530005365610123, 0.004638617858290672, 0.0064698499627411366, 0.000468690472189337, 0.0019087897380813956, 0.003993742633610964, 0.006773876491934061, 0.003893646877259016, 0.09184964001178741, 0.0, 0.0, 0.0, 0.0], [0.7321850657463074, 0.026616619899868965, 0.021639272570610046, 0.0017235631821677089, 0.0014296957524493337, 0.00095965969376266, 0.017532994970679283, 0.026055388152599335, 0.0015785368159413338, 0.007342527154833078, 0.007520848419517279, 0.004199888091534376, 0.0018326423596590757, 0.03848867863416672, 0.019075049087405205, 0.003438980085775256, 0.0035020774230360985, 0.08487847447395325, 0.0, 0.0, 0.0], [0.7706304788589478, 0.028087349608540535, 0.01704554259777069, 0.004126549698412418, 0.000675021845381707, 0.0018607713282108307, 0.02323451265692711, 0.01164253894239664, 0.0014582305448129773, 0.003796669887378812, 0.009275865741074085, 0.00416685501113534, 0.0018101718742400408, 0.0038340804167091846, 0.013590188696980476, 0.001210880232974887, 0.0007870507542975247, 0.0017650843365117908, 0.1010022908449173, 0.0, 0.0], [0.8081783056259155, 0.018046969547867775, 0.03158250451087952, 0.005772571079432964, 0.006041115615516901, 0.0025381455197930336, 0.014035630971193314, 0.034848302602767944, 0.0034456481225788593, 0.01796816475689411, 0.008775989525020123, 0.01657215692102909, 0.0024943570606410503, 0.003781798528507352, 0.004553833045065403, 0.004199001472443342, 0.0045478674583137035, 0.002657301491126418, 0.004181929398328066, 0.0057784393429756165, 0.0], [0.7137814164161682, 0.029553759843111038, 0.013558256439864635, 0.002084199106320739, 0.0005692737177014351, 0.0008874117629602551, 0.026443442329764366, 0.043409284204244614, 0.0006580944173038006, 0.011697688139975071, 0.004180937074124813, 0.0021588755771517754, 0.001450197072699666, 0.004953017923980951, 0.019172299653291702, 0.005274155177175999, 0.004623490385711193, 0.0008242049952968955, 0.0021751534659415483, 0.015267745591700077, 0.09727709740400314]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8884233832359314, 0.11157659441232681, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6246975064277649, 0.07242763787508011, 0.3028748631477356, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34219852089881897, 0.2340623438358307, 0.3043832778930664, 0.11935582756996155, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19367441534996033, 0.17396503686904907, 0.34507688879966736, 0.2146667093038559, 0.07261689752340317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14604328572750092, 0.15216629207134247, 0.15093502402305603, 0.3285096287727356, 0.13336212933063507, 0.08898361027240753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08761759847402573, 0.1960480511188507, 0.5147249698638916, 0.054698143154382706, 0.028152985498309135, 0.08698252588510513, 0.03177567571401596, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17491625249385834, 0.06440325081348419, 0.19589218497276306, 0.03003334440290928, 0.025615427643060684, 0.066634401679039, 0.31274160742759705, 0.1297636181116104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06422208994626999, 0.0295121967792511, 0.1324826180934906, 0.14466296136379242, 0.06725630164146423, 0.09168379753828049, 0.10852208733558655, 0.3131325840950012, 0.04852529615163803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06696899235248566, 0.25405341386795044, 0.07428458333015442, 0.10757417976856232, 0.045386746525764465, 0.12563195824623108, 0.03960982337594032, 0.1482328325510025, 0.022018322721123695, 0.11623915284872055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018608594313263893, 0.04072189703583717, 0.018779773265123367, 0.04298493638634682, 0.030315617099404335, 0.01305734645575285, 0.03671478107571602, 0.09327539056539536, 0.5961179137229919, 0.07677400857210159, 0.032649774104356766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09227338433265686, 0.02290245145559311, 0.04407186433672905, 0.019841155037283897, 0.022412195801734924, 0.033113885670900345, 0.0407315231859684, 0.23239262402057648, 0.02754473127424717, 0.13937172293663025, 0.1889674961566925, 0.13637696206569672, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08726327866315842, 0.03663160279393196, 0.050098780542612076, 0.04039732366800308, 0.019527694210410118, 0.03464655950665474, 0.0468786358833313, 0.09356627613306046, 0.02000322751700878, 0.13488228619098663, 0.0563649944961071, 0.2146967649459839, 0.16504265367984772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07693903148174286, 0.028835171833634377, 0.0189287681132555, 0.02718869037926197, 0.016251539811491966, 0.019975535571575165, 0.026264745742082596, 0.06901007145643234, 0.0021154938731342554, 0.1359391063451767, 0.017090914770960808, 0.22882014513015747, 0.25788429379463196, 0.07475648075342178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.055563557893037796, 0.01713678613305092, 0.04249696061015129, 0.05617745965719223, 0.01277565024793148, 0.022443661466240883, 0.017350832000374794, 0.0551413930952549, 0.018979491665959358, 0.032482534646987915, 0.05398812144994736, 0.22916191816329956, 0.2335500568151474, 0.1190740168094635, 0.033677585422992706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08635807037353516, 0.04073112830519676, 0.022123586386442184, 0.03634601831436157, 0.014002399519085884, 0.024787520989775658, 0.01590169593691826, 0.07946489751338959, 0.003450532676652074, 0.046452637761831284, 0.020262397825717926, 0.1902015656232834, 0.11868640035390854, 0.1875147819519043, 0.05447051301598549, 0.05924581363797188, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03433224558830261, 0.009003656916320324, 0.013146738521754742, 0.011143931187689304, 0.007415255531668663, 0.011112411506474018, 0.01203943695873022, 0.04737439751625061, 0.004340691491961479, 0.05085126310586929, 0.01818278804421425, 0.08031066507101059, 0.11614510416984558, 0.13364063203334808, 0.02516218274831772, 0.17862914502620697, 0.24716953933238983, 0.0, 0.0, 0.0, 0.0], [0.03589436039328575, 0.008766881190240383, 0.0073799979873001575, 0.011703106574714184, 0.008077850565314293, 0.012475458905100822, 0.017073074355721474, 0.02794327214360237, 0.0011547765461727977, 0.051392655819654465, 0.008067053742706776, 0.054979752749204636, 0.10926822572946548, 0.089189313352108, 0.04023138806223869, 0.13130922615528107, 0.3281729221343994, 0.05692070722579956, 0.0, 0.0, 0.0], [0.0590033158659935, 0.014715173281729221, 0.013175773434340954, 0.029755843803286552, 0.010262240655720234, 0.010204898193478584, 0.014005926437675953, 0.05902669578790665, 0.015005476772785187, 0.055050574243068695, 0.032016441226005554, 0.04167739674448967, 0.03033101186156273, 0.09970305114984512, 0.018768485635519028, 0.04391179978847504, 0.23909704387187958, 0.1495361477136612, 0.06475271284580231, 0.0, 0.0], [0.06819122284650803, 0.01238014455884695, 0.02887151762843132, 0.007986599579453468, 0.008407244458794594, 0.012230443768203259, 0.020754290744662285, 0.037361469119787216, 0.004182540345937014, 0.04466432332992554, 0.017961984500288963, 0.03950517624616623, 0.07486362755298615, 0.050759900361299515, 0.02857057750225067, 0.11083469539880753, 0.21274526417255402, 0.08677422255277634, 0.04373416677117348, 0.0892205759882927, 0.0], [0.030621767044067383, 0.01002620067447424, 0.0046355570666491985, 0.010692971758544445, 0.008328159339725971, 0.007036249153316021, 0.01470270287245512, 0.020653678104281425, 0.0033402855042368174, 0.01385218370705843, 0.007075910922139883, 0.02158353291451931, 0.03120276890695095, 0.09804131835699081, 0.017845576629042625, 0.040556035935878754, 0.18355591595172882, 0.10481094568967819, 0.10425786674022675, 0.0970083624124527, 0.17017194628715515]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993168830871582, 0.006831125821918249, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9909125566482544, 0.0048058172687888145, 0.004281558562070131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9411077499389648, 0.004420813173055649, 0.006985086016356945, 0.0474863164126873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9130054116249084, 0.01270272582769394, 0.010666919872164726, 0.041922055184841156, 0.021702930331230164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7471420764923096, 0.005543042439967394, 0.006296972744166851, 0.1018943041563034, 0.09326677024364471, 0.045856866985559464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.749762237071991, 0.009507793001830578, 0.03496064990758896, 0.07731331139802933, 0.055031973868608475, 0.05746903270483017, 0.015955088660120964, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8982064723968506, 0.006390889175236225, 0.004366201348602772, 0.04240674525499344, 0.009139961563050747, 0.021363668143749237, 0.016953416168689728, 0.0011726366356015205, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.652225911617279, 0.01318296603858471, 0.04246750473976135, 0.07619138062000275, 0.059148143976926804, 0.14032316207885742, 0.010752463713288307, 0.0032736356370151043, 0.0024348876904696226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7474308609962463, 0.015030000358819962, 0.007387874647974968, 0.05092277750372887, 0.03221724554896355, 0.0747595950961113, 0.029267359524965286, 0.0032132393680512905, 0.003910545259714127, 0.03586050868034363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6815649271011353, 0.012024112977087498, 0.033293623477220535, 0.07592674344778061, 0.031971100717782974, 0.08676628023386002, 0.012052803300321102, 0.004798527806997299, 0.004132622852921486, 0.05306639522314072, 0.004402805585414171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6565271019935608, 0.016260279342532158, 0.010471056215465069, 0.09563296288251877, 0.011725865304470062, 0.05593407526612282, 0.02184240147471428, 0.005105690099298954, 0.004446149803698063, 0.10462155938148499, 0.0042577884159982204, 0.013175026513636112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8589608073234558, 0.01826036348938942, 0.005236344877630472, 0.010184912011027336, 0.04041977599263191, 0.02898860163986683, 0.006681517697870731, 0.001074634725227952, 0.0014460819074884057, 0.01731308363378048, 0.002915098564699292, 0.006912103854119778, 0.0016066456446424127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.680565357208252, 0.011416928842663765, 0.010216807946562767, 0.0915243923664093, 0.01743052341043949, 0.048833414912223816, 0.023416724056005478, 0.002330429619178176, 0.005071838852018118, 0.034837376326322556, 0.004028406925499439, 0.04323097690939903, 0.0176618080586195, 0.009435044601559639, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.658906877040863, 0.010464641265571117, 0.028432220220565796, 0.045294638723134995, 0.04799739271402359, 0.08022209256887436, 0.010369955562055111, 0.0018804039573296905, 0.0018258310155943036, 0.05579482018947601, 0.0030108147766441107, 0.012328633107244968, 0.015818586573004723, 0.008909285068511963, 0.018743859604001045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5519060492515564, 0.0116317393258214, 0.005874207708984613, 0.055241696536540985, 0.026832034811377525, 0.07384233921766281, 0.02945372834801674, 0.002515605418011546, 0.004941142629832029, 0.09308967739343643, 0.005736660212278366, 0.022274397313594818, 0.01995272934436798, 0.027532421052455902, 0.0448976531624794, 0.024277912452816963, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7556489706039429, 0.022019125521183014, 0.01344206277281046, 0.03637620806694031, 0.014529034495353699, 0.026208041235804558, 0.013477290980517864, 0.002144000492990017, 0.0035461706575006247, 0.01570061407983303, 0.0032315466087311506, 0.019623083993792534, 0.019538650289177895, 0.009367870166897774, 0.021053515374660492, 0.012438991107046604, 0.011654834263026714, 0.0, 0.0, 0.0, 0.0], [0.5184034705162048, 0.010390885174274445, 0.0072076874785125256, 0.10272296518087387, 0.03690440580248833, 0.0643257424235344, 0.021884074434638023, 0.0026022428646683693, 0.004734701476991177, 0.05344801023602486, 0.00416689133271575, 0.039253536611795425, 0.028164176270365715, 0.009787634015083313, 0.03201041370630264, 0.040606897324323654, 0.01718846522271633, 0.006197826471179724, 0.0, 0.0, 0.0], [0.5519921779632568, 0.010934006422758102, 0.0038671328220516443, 0.06785749644041061, 0.021463429555296898, 0.050938211381435394, 0.0205681249499321, 0.002696097129955888, 0.004062270745635033, 0.06531991809606552, 0.005494627170264721, 0.0237941425293684, 0.0730578750371933, 0.015011278912425041, 0.02775847725570202, 0.028324462473392487, 0.015557697974145412, 0.003690646030008793, 0.007611986715346575, 0.0, 0.0], [0.6240167021751404, 0.005970961879938841, 0.008620118722319603, 0.12044164538383484, 0.01747366413474083, 0.03250548988580704, 0.01204650942236185, 0.002561987843364477, 0.0022828183136880398, 0.023543264716863632, 0.002360305981710553, 0.03353310376405716, 0.03466283529996872, 0.008584539406001568, 0.007306418847292662, 0.02871811017394066, 0.01693265326321125, 0.0022265228908509016, 0.011629045940935612, 0.004583260975778103, 0.0], [0.6444841027259827, 0.00878015998750925, 0.004652759525924921, 0.037385545670986176, 0.01368277333676815, 0.033788055181503296, 0.0247152391821146, 0.0030623150523751974, 0.0056511410512030125, 0.032053135335445404, 0.008720314130187035, 0.04327044263482094, 0.03222762420773506, 0.010904775932431221, 0.0409020371735096, 0.01404728926718235, 0.00951890554279089, 0.0028506361413747072, 0.01254237536340952, 0.007557271514087915, 0.009203142486512661]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9915279150009155, 0.008472086861729622, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9881539940834045, 0.005379415582865477, 0.00646660290658474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9361326694488525, 0.004628785885870457, 0.00991657841950655, 0.049321915954351425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8758513331413269, 0.016475355252623558, 0.028426701202988625, 0.02882968634366989, 0.05041702464222908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7041001915931702, 0.005988268181681633, 0.0011339545017108321, 0.17788934707641602, 0.09109716862440109, 0.019791092723608017, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6666878461837769, 0.013486621901392937, 0.038767289370298386, 0.14287348091602325, 0.07826194167137146, 0.05189117416739464, 0.008031638339161873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8252638578414917, 0.010457353666424751, 0.014047035947442055, 0.1085348129272461, 0.016340317204594612, 0.015974927693605423, 0.00791475735604763, 0.0014670082600787282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6184839606285095, 0.012532071210443974, 0.042486608028411865, 0.13343167304992676, 0.09058075398206711, 0.08947502821683884, 0.008581269532442093, 0.003190217074006796, 0.001238350523635745, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6926917433738708, 0.026862137019634247, 0.011271446943283081, 0.07036497443914413, 0.033176738768815994, 0.12263254821300507, 0.027061525732278824, 0.002222177805379033, 0.0029728214722126722, 0.010743880644440651, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6442015171051025, 0.01855689100921154, 0.03167925402522087, 0.10904451459646225, 0.06152728945016861, 0.09928525984287262, 0.014781600795686245, 0.002465564291924238, 0.002361318562179804, 0.014086432754993439, 0.0020104378927499056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7232524752616882, 0.03990674763917923, 0.028032569214701653, 0.08214981108903885, 0.03099115379154682, 0.03296284377574921, 0.0051489463075995445, 0.005735544953495264, 0.0015162820927798748, 0.03543424606323242, 0.0008641120512038469, 0.014005257748067379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7600786685943604, 0.025904346257448196, 0.07377003878355026, 0.011386743746697903, 0.03935076668858528, 0.02355354279279709, 0.04520353674888611, 0.0014431406743824482, 0.000504974857904017, 0.007104312069714069, 0.002246905816718936, 0.007135499268770218, 0.0023175161331892014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7675660848617554, 0.0057425545528531075, 0.012349111028015614, 0.08150386065244675, 0.016261128708720207, 0.04435252770781517, 0.004768615122884512, 0.0006297772633843124, 0.0014904240379109979, 0.011834090575575829, 0.0011965418234467506, 0.01299181580543518, 0.03342023491859436, 0.005893136374652386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6587777733802795, 0.010142819955945015, 0.07439668476581573, 0.060498252511024475, 0.05417380481958389, 0.05387463420629501, 0.01129530556499958, 0.0024281914811581373, 0.0006416513933800161, 0.006573587656021118, 0.0015825056470930576, 0.006498824805021286, 0.03699144348502159, 0.020929325371980667, 0.0011952035129070282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5448915958404541, 0.0659312903881073, 0.019283456727862358, 0.040367018431425095, 0.027991510927677155, 0.028489476069808006, 0.010195089504122734, 0.005437583662569523, 0.0012233996530994773, 0.042195048183202744, 0.00229261489585042, 0.014710459858179092, 0.027308572083711624, 0.10785914957523346, 0.0040358891710639, 0.05778784304857254, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5722500085830688, 0.038980502635240555, 0.17126871645450592, 0.024177907034754753, 0.016027003526687622, 0.024853540584445, 0.028051497414708138, 0.0016214033821597695, 0.0009552477858960629, 0.008146598003804684, 0.0023558062966912985, 0.007103289477527142, 0.011031552217900753, 0.021326450631022453, 0.001064071198925376, 0.022954266518354416, 0.04783215746283531, 0.0, 0.0, 0.0, 0.0], [0.582019567489624, 0.0046808659099042416, 0.013190152123570442, 0.09067091345787048, 0.018670756369829178, 0.019760191440582275, 0.003413804806768894, 0.0009022997110150754, 0.0010488162515684962, 0.004284215159714222, 0.0006869487697258592, 0.009554244577884674, 0.026036549359560013, 0.010181605815887451, 0.0011806234251707792, 0.10935267060995102, 0.0939587950706482, 0.01040692999958992, 0.0, 0.0, 0.0], [0.45996585488319397, 0.014155306853353977, 0.012511657550930977, 0.09011417627334595, 0.023827603086829185, 0.021275104954838753, 0.008453642949461937, 0.003423010464757681, 0.0022542874794453382, 0.036438606679439545, 0.001072758692316711, 0.009355122223496437, 0.044250667095184326, 0.007795086596161127, 0.004411100875586271, 0.10051856935024261, 0.1429368406534195, 0.010801359079778194, 0.006439269054681063, 0.0, 0.0], [0.5318396091461182, 0.005233260802924633, 0.013229005970060825, 0.14284400641918182, 0.01989649422466755, 0.022930774837732315, 0.0026265045162290335, 0.0018087701173499227, 0.0005677806911990047, 0.0049186344258487225, 0.0006731781177222729, 0.011225352995097637, 0.048253074288368225, 0.011223752051591873, 0.0006160886259749532, 0.08232691884040833, 0.08671214431524277, 0.004298918880522251, 0.004887476563453674, 0.003888198174536228, 0.0], [0.5570634603500366, 0.02556406706571579, 0.048259615898132324, 0.0649782195687294, 0.010203625075519085, 0.010430889204144478, 0.007821771316230297, 0.0046341801062226295, 0.0018935143016278744, 0.012444527819752693, 0.0007817979785613716, 0.018280889838933945, 0.030499424785375595, 0.018913334235548973, 0.004282189533114433, 0.017399627715349197, 0.10421489924192429, 0.010606608353555202, 0.010463877581059933, 0.015411282889544964, 0.025852207094430923]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9675114750862122, 0.032488517463207245, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8937566876411438, 0.039290525019168854, 0.06695283204317093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9249371290206909, 0.020321493968367577, 0.035676125437021255, 0.01906535215675831, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.925191342830658, 0.025370780378580093, 0.021357977762818336, 0.019340164959430695, 0.008739813230931759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8208416700363159, 0.034711748361587524, 0.04559040442109108, 0.026170644909143448, 0.0516284815967083, 0.02105710282921791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5897791385650635, 0.036701787263154984, 0.06043042242527008, 0.06264380365610123, 0.06982595473527908, 0.07463546842336655, 0.10598345100879669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6481769680976868, 0.03317893669009209, 0.06676458567380905, 0.061146460473537445, 0.02142736129462719, 0.03776220604777336, 0.050107598304748535, 0.08143582940101624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5681624412536621, 0.026810020208358765, 0.031691815704107285, 0.043075669556856155, 0.10618443787097931, 0.0688229352235794, 0.04915230721235275, 0.0655917227268219, 0.040508661419153214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5708568692207336, 0.04368804395198822, 0.030283158645033836, 0.03306606784462929, 0.031437672674655914, 0.032956384122371674, 0.0549742616713047, 0.123879574239254, 0.04757535085082054, 0.031282566487789154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6147634983062744, 0.03612162545323372, 0.038691725581884384, 0.023604214191436768, 0.03519275784492493, 0.021874405443668365, 0.04038963466882706, 0.09398751705884933, 0.03422683849930763, 0.020318202674388885, 0.04082966595888138, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.423816055059433, 0.03733909875154495, 0.04732673242688179, 0.03330674394965172, 0.03076932579278946, 0.026177866384387016, 0.06482280790805817, 0.1332140862941742, 0.049772538244724274, 0.04036592319607735, 0.06691975146532059, 0.04616912826895714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7805125713348389, 0.018170839175581932, 0.013213560916483402, 0.01849021576344967, 0.02221662551164627, 0.017407884821295738, 0.02389334701001644, 0.024112338200211525, 0.017009301111102104, 0.024649064987897873, 0.01969529315829277, 0.017159169539809227, 0.003469721879810095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4571433663368225, 0.03493153303861618, 0.04141158610582352, 0.029900498688220978, 0.018516618758440018, 0.027482852339744568, 0.052782073616981506, 0.10939162224531174, 0.03688905015587807, 0.03655894845724106, 0.04950214922428131, 0.04073203727602959, 0.041604917496442795, 0.023152794688940048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6244637966156006, 0.02300231344997883, 0.018397552892565727, 0.02879221923649311, 0.05032098665833473, 0.03309575468301773, 0.026576757431030273, 0.04153040423989296, 0.017778683453798294, 0.03222151845693588, 0.023414399474859238, 0.028256656602025032, 0.023896312341094017, 0.013676527887582779, 0.014576086774468422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30003196001052856, 0.0293098334223032, 0.026353225111961365, 0.018568560481071472, 0.04477650672197342, 0.02918585203588009, 0.05826956406235695, 0.12557166814804077, 0.045474424958229065, 0.04439342021942139, 0.07453282177448273, 0.029597239568829536, 0.03159515559673309, 0.03853514790534973, 0.06485779583454132, 0.038946814835071564, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4191756248474121, 0.03024584800004959, 0.04639191925525665, 0.024745188653469086, 0.01645757630467415, 0.021656380966305733, 0.04515845701098442, 0.11030847579240799, 0.025378914549946785, 0.045798949897289276, 0.04147309437394142, 0.03950845077633858, 0.014928110875189304, 0.028294222429394722, 0.030393186956644058, 0.028653396293520927, 0.03143223002552986, 0.0, 0.0, 0.0, 0.0], [0.33674734830856323, 0.027449311688542366, 0.03845040872693062, 0.026380127295851707, 0.02451181225478649, 0.021527908742427826, 0.048378556966781616, 0.09518412500619888, 0.0347689650952816, 0.044185079634189606, 0.04896198585629463, 0.0490235798060894, 0.02771615982055664, 0.02668629214167595, 0.06260983645915985, 0.03921007364988327, 0.031363196671009064, 0.016845203936100006, 0.0, 0.0, 0.0], [0.36636921763420105, 0.021028421819210052, 0.023092826828360558, 0.017683688551187515, 0.02228429727256298, 0.02151552028954029, 0.04363572597503662, 0.08290868252515793, 0.0397423692047596, 0.02915385365486145, 0.06588444858789444, 0.029914848506450653, 0.03573649004101753, 0.032349735498428345, 0.04911195486783981, 0.024737516418099403, 0.02577933482825756, 0.03055097907781601, 0.03852004185318947, 0.0, 0.0], [0.44727620482444763, 0.020807571709156036, 0.033530429005622864, 0.02878640592098236, 0.01929902471601963, 0.02653326466679573, 0.037884149700403214, 0.06142016127705574, 0.01903720386326313, 0.03336197882890701, 0.024160683155059814, 0.04300522059202194, 0.01972767524421215, 0.02901095151901245, 0.023126166313886642, 0.030487600713968277, 0.028043750673532486, 0.01461291778832674, 0.03232444077730179, 0.0275641530752182, 0.0], [0.21800841391086578, 0.01553725078701973, 0.021484537050127983, 0.01683085598051548, 0.01915995217859745, 0.02328585833311081, 0.05463326722383499, 0.10984614491462708, 0.039398372173309326, 0.038988564163446426, 0.06502760946750641, 0.03116570971906185, 0.024682534858584404, 0.037149328738451004, 0.04708231985569, 0.03805147483944893, 0.03532388061285019, 0.03435606509447098, 0.03805634006857872, 0.03445535898208618, 0.057476211339235306]]], \"attentionHeadNames\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"], \"tokens\": [\"<|BOS|>\", \" in\", \" the\", \" POP\", \"v\", \"X\", \".\", \" This\", \" '\", \"data\", \"'\", \" file\", \" sw\", \"apping\", \"/\", \"packet\", \" sniff\", \"ing\", \" type\", \" of\", \" technique\"]}\n",
       "    )\n",
       "    </script></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OV Shape:  torch.Size([8, 512, 512])\n",
      "pre attention scores:  torch.Size([21, 512])\n",
      "Headwise OV Scores:  torch.Size([8, 21, 512])\n",
      "Direct Attribution Scores Shape:  torch.Size([8, 21])\n"
     ]
    }
   ],
   "source": [
    "# torch.set_printoptions(precision=4, sci_mode=False)\n",
    "j = 20 #? this is the destination token\n",
    "\n",
    "W_OV = einops.einsum(model.W_V[0], model.W_O[0], \"head d1 hdim, head hdim d2 -> head d1 d2\")\n",
    "print(\"OV Shape: \", W_OV.shape)\n",
    "\n",
    "x_pre = cache['blocks.0.ln1.hook_normalized'][0]\n",
    "print(\"pre attention scores: \", x_pre.shape)\n",
    "\n",
    "headwise_OV_scores = einops.einsum(x_pre, W_OV, \"pos dim1, head dim1 dim2 -> head pos dim2\")\n",
    "print(\"Headwise OV Scores: \", headwise_OV_scores.shape)\n",
    "\n",
    "direct_score_pre_attn = einops.einsum(n_mid, headwise_OV_scores,  \"dim, head pos dim -> head pos\")\n",
    "direct_score_pst_attn = einops.einsum(direct_score_pre_attn, cache['blocks.0.attn.hook_pattern'][0, :, j], \"head pos, head pos -> head pos\")\n",
    "print(\"Direct Attribution Scores Shape: \", direct_score_pst_attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(utils.to_numpy(direct_score_pst_attn.detach().cpu()), \n",
    "            color_continuous_midpoint=0.0, \n",
    "            color_continuous_scale=\"RdBu\", \n",
    "            labels={\"x\": \"Position\", \"y\": \"Head\"}, \n",
    "            title=\"Direct Attribution Scores\",\n",
    "            x=[f\"{tok} {i}\" for i, tok in enumerate(str_tokens)] )\n",
    "\n",
    "# writing to a file since show wasn't working in vscode properly\n",
    "fig.write_image(\"direct_score_attn_heads.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " virtual\n"
     ]
    }
   ],
   "source": [
    "print(str_tokens[16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform de-embedding for the OV Circuis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headwise Pre Vector Shape:  torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "headwise_pre_attn_vector = einops.einsum(n_mid, W_OV, \"dim2, head dim1 dim2 -> head dim1\")\n",
    "print(\"Headwise Pre Vector Shape: \", headwise_pre_attn_vector.shape)\n",
    "\n",
    "# perform de-embedding of the token\n",
    "direct_attn_scores = einops.einsum(model.W_E, headwise_pre_attn_vector, \"n_vocab dim, head dim -> head n_vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_57c53_row0_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_57c53_row1_col1 {\n",
       "  background-color: #c4d5f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_57c53_row2_col1 {\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_57c53_row3_col1 {\n",
       "  background-color: #8db0fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_57c53_row4_col1 {\n",
       "  background-color: #465ecf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_57c53_row5_col1 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_57c53_row6_col1 {\n",
       "  background-color: #4055c8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_57c53_row7_col1 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_57c53_row8_col1, #T_57c53_row9_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_57c53\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_57c53_level0_col0\" class=\"col_heading level0 col0\" >token</th>\n",
       "      <th id=\"T_57c53_level0_col1\" class=\"col_heading level0 col1\" >feature_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row0\" class=\"row_heading level0 row0\" >274</th>\n",
       "      <td id=\"T_57c53_row0_col0\" class=\"data row0 col0\" >·of</td>\n",
       "      <td id=\"T_57c53_row0_col1\" class=\"data row0 col1\" >0.589367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row1\" class=\"row_heading level0 row1\" >4539</th>\n",
       "      <td id=\"T_57c53_row1_col0\" class=\"data row1 col0\" >·Of</td>\n",
       "      <td id=\"T_57c53_row1_col1\" class=\"data row1 col1\" >0.329345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row2\" class=\"row_heading level0 row2\" >4387</th>\n",
       "      <td id=\"T_57c53_row2_col0\" class=\"data row2 col0\" >Of</td>\n",
       "      <td id=\"T_57c53_row2_col1\" class=\"data row2 col1\" >0.269217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row3\" class=\"row_heading level0 row3\" >1153</th>\n",
       "      <td id=\"T_57c53_row3_col0\" class=\"data row3 col0\" >of</td>\n",
       "      <td id=\"T_57c53_row3_col1\" class=\"data row3 col1\" >0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row4\" class=\"row_heading level0 row4\" >40537</th>\n",
       "      <td id=\"T_57c53_row4_col0\" class=\"data row4 col0\" >·στο</td>\n",
       "      <td id=\"T_57c53_row4_col1\" class=\"data row4 col1\" >0.164450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row5\" class=\"row_heading level0 row5\" >1107</th>\n",
       "      <td id=\"T_57c53_row5_col0\" class=\"data row5 col0\" >frac</td>\n",
       "      <td id=\"T_57c53_row5_col1\" class=\"data row5 col1\" >0.156970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row6\" class=\"row_heading level0 row6\" >12970</th>\n",
       "      <td id=\"T_57c53_row6_col0\" class=\"data row6 col0\" >·nas</td>\n",
       "      <td id=\"T_57c53_row6_col1\" class=\"data row6 col1\" >0.155688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row7\" class=\"row_heading level0 row7\" >17184</th>\n",
       "      <td id=\"T_57c53_row7_col0\" class=\"data row7 col0\" >·το</td>\n",
       "      <td id=\"T_57c53_row7_col1\" class=\"data row7 col1\" >0.154302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row8\" class=\"row_heading level0 row8\" >7496</th>\n",
       "      <td id=\"T_57c53_row8_col0\" class=\"data row8 col0\" >·san</td>\n",
       "      <td id=\"T_57c53_row8_col1\" class=\"data row8 col1\" >0.145928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_57c53_level0_row9\" class=\"row_heading level0 row9\" >3368</th>\n",
       "      <td id=\"T_57c53_row9_col0\" class=\"data row9 col0\" >·OF</td>\n",
       "      <td id=\"T_57c53_row9_col1\" class=\"data row9 col1\" >0.145797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f807825c4c0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_score_df(direct_attn_scores[0], ascending=False ).head(10).style.background_gradient(\"coolwarm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermarking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
